%!TEX root=document.tex

\section{Experimental Evaluation}

In this section we describe our evaluation of \VizRecDB.
We evaluated \VizRecDB\ along various metrics on a variety of real and synthetic
datasets.
Our goals in this study were to evaluate the following: 
\squishlist
\item G1: Can \VizRecDB\ find interesting views in general?
\item G2: Can \VizRecDB\ find interesting views based on our definition of
interesting? (with and without the use of approximation heuristics)
\item G3: How fast can \VizRecDB\ find interesting views?
\item G4: How do our optimization parameters affect the performance of
\VizRecDB?
\squishend

We start with a description of the datasets used, followed by our experimental
setup and then describe our experimental results.

\subsubsection*{Datasets}
The results and performance
for a tool like \VizRecDB\ depend closely on the dataset being studied and its
data distribution.
Therefore, for evaluating \VizRecDB\ we chose a diverse set of datasets.
These datasets were obtained from the UCI repository and augmented by
external data sources. 
We selected a representative set of datasets from the UCI repository that
contained mixed attributes (i.e. categorical and numeric).
We classify our test data along two axes: {\it the size of the dataset} and the
{\it number of views} possible in the data. 
Recall that the number of possible views is equal to the product of the number
of measure attributes and dimension attributes.
These metrics group datasets into four classes: {\it small size, few views}
(Num views: ~10, Num rows: $<$ 10K), {\it large size, few
views} (Num views: ~10, Num rows: $>$ 100K), {\it small size, many views}
(Num views: $>$100, Num rows: $<$ 10K) and {\it large size, many views} (Num
views: $>$100, Num rows: $>$ 100K).
We also consider a final class of datasets called {\it very large size, many
views} which corresponds to datasets with $>$ 100 views and tens of millions of
rows, having sizes of multiple GBs.
There are of course datasets that fall in between but we focus on
these classes of datasets to discover trends in the performance of \VizRecDB.
Another axis along which we compare datasets is the amount of variation in
their view utilities.
This property is hard to control in real datasets and therefore we constructed
synthetic datasets with varying amounts of variation in view utility (see
Appendix \ref{} for techniques).
Table \ref{tab:datasets} shows the datasets in each of the
above categories that we test with.

\begin{table}[htb]
  \centering \scriptsize
  \begin{tabular}{|c|c|c|c|c|} \hline
  Dataset type & Name & Description & Size & Views \\ \hline
  Small size, & Abalone dataset  & Physical Measurements  & 4K & 14 \\
  Few Views & (ABA) & of Abalone & & \\ \cline{2-5}
  & Contraceptive Method  & Contraceptive & 1.5K &
  16 \\
  & (CONT) & Methods Survey & & \\ \hline 
  Large size, & Synthetic dataset  & Low variation  & 1M & 10 \\
  Few Views & (SYN1) & in view utility & & \\ \cline{2-5}
  & Synthetic dataset  & High variation & 1M &
  10 \\
  & (SYN1*) & in view utility & & \\ \hline 
  Small size, & Internet Ads  & Features of ads  & 3K & 1.5K* \\
  Many Views & (ADS) & & & \\ \cline{2-5}
  & Insurance Data  & Customer data & 9K &
  1500* \\
  & (INS) & & & \\ \hline 
  Large size, & Bank dataset  & Customer Loan  & 40K & 80* \\
  Many Views & (BNK) & Information & & \\ \cline{2-5}
  & Census Data  & Demographic & 300K &
  200* \\
  & (CEN) & Information & & \\ \cline{2-5}
  & Cover Type  & Forest cover & 500K &
  100* \\
  & (COV) & & & \\ \cline{2-5}
  & Diabetes data  & Hospital data & 100K &
  80* \\
  & (DIAB) & about diabetics & &\\ \hline 
  Very Large size, & Synthetic dataset  & Low variation  & 10M & 100 \\
  Many Views & (SYN2) & in view utility & & \\ \cline{2-5}
  & Synthetic dataset  & High variation & 10M &
  100 \\
  & (SYN2*) & in view utility & & \\ \hline 
  \end{tabular}
  \caption{Datasets used for testing}
  \label{tab:datasets} 
\end{table}

% In particular, we evaluated the performance of both our execution engines and
% their respective optimizations., and the respective optimizations. 
% For the DBMS-backed engine, we also built an analytical model to help pick
% parameters for \VizRecDB.

\subsubsection*{Experimental Setup}

% \begin{table}[htb]
%   \centering \scriptsize
%   \begin{tabular}{|c|c|c|c|c|c|c|} \hline
%   Type & Dataset & Num  & Num  & Num  & Size (GB) &
%   Num \\
%   & Name & Rows & Dimensions &  Measures & & Views \\ \hline 
%    & $Small_1$ & 1M & 5 & 2 &  0.1 & \\ 
%   Small & $Small_2$ & 10M & 5 & 2 &  1 & 10\\ 
%    & $Small_3$ & 100M & 5 & 2 &  10 & \\ \hline
%    & $Med_1$ & 1M & 50 & 5 &  0.4 & \\
%   Medium & $Med_2$ & 10M & 50 & 5 &  4 & 250\\ 
%    & $Med_3$ & 100M & 50 & 5 &  40 & \\ \hline
%    & $Large_1$ & 1M & 100 & 10 &  1 & \\
%   Large & $Large_2$ & 10M & 100 & 10 &  10 & 1000\\
%    & $Large_3$ & 100M & 100 & 10 &  100 & \\ \hline
%   \end{tabular}
%   \caption{Datasets used for testing}
%   \label{tab:datasets} 
% \end{table}
% 
% Table \ref{tab:datasets} lists the datasets on which we evaluated the
% performance of \VizRecDB. These datasets are synthetically generated and their size
% varies from 100 MB to 100 GB and number of attributes ranges from 5 - 100 dimension attributes and 2 -
%  10 measure attributes. The relative cardinality of dimension and measure attributes
% was chosen to model real-world datasets which usually have a large number of
% dimension attributes but few measure attributes. To accurately estimate the
% effect of specific optimizations, for each dataset, we also created a
% supplemental dataset with the same specifications except that each attribute had
% the same number of distinct values (100).
% 
% For each dataset, we used \VizRecDB\ to find the top 20 views of the input query.
% Any run that took more than 1 hour was terminated. All experiments were repeated
% three times and the results were averaged.

We took the datasets described above and ran \VizRecDB\ on them using different
settings.
To evaluate the different experimental engines, each experiment was run in three
environments: (a) \VizRecDB\ backed by a row store, (b) \VizRecDB\ backed by a
column store, and (c) \VizRecDB\ with our custom execution engine.
For each experiment, we varied the number of top views to be selected, the
selectivity of the query, and implementation specific parameters for each
execution engine.

Each run was evaluated along the following dimensions: latency, precision and
recall. 
Latency measured the time to compute each run. Each experiment was
repeated three times and the execution times were averaged.
For heuristics, precision measured
how many of the top views generated by the algorithm were true positives
(compared to the ground truth) and recall measured how many of the true top-$k$
views were retrieved by the algorithm.
We also measured the {\it utility difference}, i.e., the numeric differences
between the average utility of the true top-$k$ views and the average utility of
the algorithm-generated top-$k$ views.

All experiments were run on a single
machine with XXX Intel Xeon E7 processors with 8 GB
RAM.

We frame the rest of the section according to the goals of our study mentioned
above.
We begin with G1 and G2: can \VizRecDB\ identify interesting views using our
definition of interesting? Then we visit G4 and study the effect of
optimizations on the performance of \VizRecDB. 
Finally, we use optimal parameters obtained in the previous step to
address G4 and evaluate performance of all our techniques.

% Before we present our results for the datasets listed above, we present an
% evaluation of the various optimizations presented in Section \ref{}.
% The goal to show the relative advantages of each optimization and choose the
% optimal set of optimizations when executing on our test datasets.

\subsection{Does \VizRecDB\ produce interesting views? (aka Correctness)}
\label{sec:expt_accuracy}

As discussed previously, there are many possible definitions of ``interesting''
views, and in \VizRecDB\ we choose a metric based on deviation as captured by
the view utility, and select the views with highest utility.

In this set of experiments, we evaluate the correctness of our techniques.
Our ground truth is produced by exhaustively
evaluating all possible views, ordering them by utility and picking the top-$k$. 
We use this ground truth to measure the precision, recall and
utility difference for each of our techniques.
Note that when \VizRecDB\ uses the DBMS-backed execution engine, it
evaluates all views and therefore it essentially computes the ground truth.
As a result, this technique necessarily produces results with perfect precision,
recall and zero utility difference.
The heuristics developed in Section \ref{}, namely
confidence interval pruning and the multi armed bandit pruning, however, do not.

To measure the impact of our heuristics on correctness, we ran our heuristics on
the datasets from Table \ref{} having {\it many views}. 
For each dataset, we ran our pruning heuristics for $k$=1\ldots 20. 
For confidence interval pruning we varied the $z$ score that controls the
confidence interval from 50\% confidence to almost 100\% confidence, and for MAB
pruning, we varied the number of phases between 10 and 1000.
Figure \ref{} shows results of our experiments with the diabetes (DIA) dataset
having 80 views and 100K rows. We see that XXX.


<<Figures>>

The pruning heuristics developed depend closely on how the view utilities are
distributed; if the view utilities are all similar, pruning will have very low
power, however, if the utilities have a high variance, we expect that more
stringent pruning will also give us high quality results.
In Figure \ref{}, we show results from experiments where we ran our heuristics
on synthetic data with different view utility distributions, particularly
datasets SYN2, SYN2*, SYN2**. The figures show that XXX.

<<Figures>>

Note that the parameters we tune for heuristics have a bearing on the latency of
\VizRecDB. We revisit this issue in the next set of experiments.


\subsection{How do our optimizations affect the performance of
\VizRecDB?}
\label{sec:expt_accuracy}

In this set of experiments, we study how the parameters of our various
algorithms affect \VizRecDB. 

\subsubsection*{Custom Execution Engine}
We begin by studying the performance of our custom engine and our pruning
heuristics. 
Figure \ref{} shows a latency comparison between the no pruning (NO\_PR) version
of our custom engine and the confidence interval (CI) and MAB (MAB) pruning
versions.
For both heuristics, we choose the $z$ score and number of phases to maximize
accuracy.
From the figure we clearly see that pruning views significantly reduces
latency.
In fact, for $k$=10, we see that CI pruning reduces latency by 50\% for all
datasets.
As shown in Section \ref{}, our custom execution engine has a simple,
moderately-optimized implementation that is centered around scanning data and
periodically pruning views.
If we see such significant gains in our simple implementation, we hypothesize
that {\it we can expect to get similar gains by incorporating this
implementation in a DBMS}.

Next, we revisit the tradeoff between latency and accuracy. 
Figure \ref{} shows a graph of latency vs. accuracy for confidence interval
pruning while Figure \ref{} shows a graph of latency vs. accuracy for MAB
pruning.
As we increase the $z$ score for confidence interval pruning increases, the
accuracy of our algorithm increases.
However, this enlargement of the utility confidence intervals reduces our
pruning power, and therefore leads to larger latencies.
Similarly, as we increase the number of phases used in MAB pruning, we increase
our pruning power. 
This decreases the latency of \VizRecDB\ but reduces the accuracy of our
algorithms.
CI pruning generally has lower latency and higher accuracy compared to MAB
pruning. 

\subsubsection*{DBMS-backed Execution Engine}

We now present experimental characterization of the optimization
strategies described for the DBMS-backed execution engine.

First, we study the performance of the DBMS-backed execution engine without any
optimizations. 
This setup corresponds to sequentially executing queries for each view.
Once again, we focus on datasets of type {\it Large size, Many views}.
Since the optimizations in this version of the execution engine are performance
based, we use (versions of?) synthetic datasets SYN3, SYN3* which have 1M rows
and 250 views.
We evaluate the engines on real data in the next set of experiments.
We run all experiments on a row store as well as a column store and provide
numbers for both environments.

 Figure \ref{} shows the average latency for SYN3 when we vary the number of
 rows and the number of views.
 As expected, the latency increases with increase in the number of views as well
 as the number of views.
 However each new view adds an entire table scan, the latency is much more
 sensitive to the number of views.
 
Next, we see how much of a gain \VizRecDB\ can obtain by combining multiple
aggregates. We denote the number of aggregates by the parameter $n_{agg}$.
Figure \ref{fig:mult_agg} shows the performance gains achieved for SYN3. 
Computing more aggregates in the same query, gives an almost linear speedup. 
We also notice that this optimization is slightly more effective for larger
datasets.
We also notice that the optimization is less effective in column stores??

{\it Combine Multiple Group-bys}
\label{sec:mult_gb_expt}

Next we evaluate the effect of combining multiple group-bys.
As discussed in Section \ref{}, combining dimension attributes with a large
number of distinct values can negatively affect performance due to high
intermediate result cost.
One implementation trick helps us deal with large intermediate results: we break
the processing into two phase. 
In the {\it temp table creation} phase, we run queries with multiple group-bys
and store the results into temporary tables.
In the second {\it temp table querying} phase, we run single aggregate and
group-by queries on the temp tables to get the final results.
For instance, suppose that we want to compute the results for views ($a_1$, $m$,
$f$), ($a_2$, $m$, $f$) and ($a_3$, $m$, $f$). Instead of executing these views
individually, we combine these three views into a single view, (\{$a_1$, $a_2$,
$a_3$\}, $m$, $f$), which computes the aggregate for attribute $m$ using
function $f$ and groups by the attributes \{$a_1$, $a_2$, $a_3$\}. The results
of this query are stored in a temporary table which is then queried to get
results for the original views. 
Notice that if we select group-by attributes right, the temp tables are much
smaller than the original table and hence are cheaper to query.

\VizRecDB\ uses the parameter $n_{GB}$ to denote the number of attributes in the
group-by clause. To evaluate the effect of combining group-bys, we ran \VizRecDB\
by varying number of group-by attributes, i.e. the $n_{GB}$ parameter, between 1
and the number of dimensions $d$ in a given table. For each run, we measured the
amount of time taken to create the temporary tables, the time taken to query the
temporary tables and the total execution time. Figure
\ref{fig:avg_tt_creation} to \ref{fig:total_time} show the
results for SYN3 1M tuples.

Figure \ref{fig:avg_tt_creation} shows the
average time required to create temporary tables for $n_{GB}$=1\ldots50. There
are several points to note in this graph: (1) for $n_{GB}>=10$, the number of
connections does not have a significant impact of the temp table creation time.
We see this behavior because for $n_{GB}>=10$, the number of temp tables created
is $<=5$ and therefore a maximum of 5 connections is used irrespective of the
number of connections that are open.
We also note an upward trend in the total temp table creation time with
increasing $n_{GB}$ because the temporary tables gradually become larger in size
(the number of rows in the table is bounded by the number of rows in the input
table but an increase in $n_{GB}$ increases the columns present in the table).
We also observe that the ``sweet spot'' for temp table creation occurs between 1 -
2 group-bys. 

% Note that the time required for temp table creation is roughly
% proportional to the size of the temp tables. We leverage this fact when we
% develop the analytical model in the next section.

\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/mult_gb_tt_creation_single.pdf} 
  \caption{Average Temp Table Creation Time} 
    \label{fig:avg_tt_creation}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/mult_gb_tt_creation_total.pdf}
  \caption{Total Temp Table Creation Time} 
    \label{fig:total_tt_creation_time}
\end{figure}

Figure \ref{fig:total_tt_creation_time} shows the total time spent in creating
temporary tables in \VizRecDB. As before, we see that the total time
flattens out after 10 group-bys; however, we observe a reordering of the the
trend lines with respect to number of connections. While the average time taken
to generate temp tables with 5 connections is the least, \VizRecDB\ must run more
batches of queries to create the required number of temporary tables. We observe
that that 40 connections is optimal for minimizing the total temp table creation
time. As in the previous diagram, we observe a minima around $n_{GB}=2$.

{\bf Temp Table Query Phase}: Figures \ref{fig:avg_tt_query_time} and
\ref{fig:total_tt_query_time} respectively show the average time required to run
each view query on the temp tables and the total time to run all view queries.
Note that for the Medium dataset, there are 250 possible views and therefore 500
view queries that are to be run against the database.

In Figure \ref{fig:avg_tt_query_time}, we see clear trends in  the average time
taken to execute view queries on temp tables. Specifically: (1) The time taken
to query a temp table increases non-linearly with the number of queries
executing in parallel. We see that the trend lines are ordered by number of
connections and the loss of performance grows with number of connections. (2) As
before, we observe a slight increase in the execution time as the size of temp
tables increases. This is not surprising since the query executor must scan and
process more data. (3) Finally, we observe a minima at $n_{GB}=2$, similar the to
two graphs above.


\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/mult_gb_tt_query_single.pdf}
  \caption{Average Temp Table Query Time}
  \label{fig:avg_tt_query_time}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/mult_gb_tt_query_total.pdf}
     \caption{Total Temp Table Query Time} 
       \label{fig:total_tt_query_time}
\end{figure}

Figure \ref{fig:total_tt_query_time} shows the total time taken to query the
temp tables for all the final views. Note again that the trends in total query
time are not identical to those in average query time because number of query
batches required is inversely proportional to the number of connections.
In Figure \ref{fig:total_tt_query_time}, we see that runs with 5 connections are
slow not because of high average query time but because of the large number of
batches of queries that must be executed. In contrast, runs with 40 connections
require very few batches but have high average query time. 10 - 20 connections
and $n_{GB}=1-2$ achieves the best performance.



{\bf Total Execution Time}: The total execution time for \VizRecDB\ is the sum of
time required for the two phases above. Figure \ref{fig:total_time} shows the
total \VizRecDB\ execution time for different values of $n_{GB}$ and number of
connections. We observe that the best performance is obtained for
$n_{GB}=2$ and 40 parallel connections.

\begin{figure}[h]
     \centering
    \includegraphics[width=6cm]{Images/mult_gb_total.pdf}
    \caption{Total Execution Time}
  \label{fig:total_time}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/mult_gb_diff_distinct.pdf}
    \caption{Total Execution Time vs. Number of Distinct Values} 
  \label{fig:total_time_diff_distinct}

\end{figure}

{\bf Effect of Number of Groups:} The above experiments suggest that $n_{GB}=2$
is the optimal value for the number of group by attributes, both for temp table
creation and querying.
Next we study whether this constraint applies to the number of attributes in the
group by clause or the number of distinct groups produced by the grouping. For
this purpose, we created variants of the Medium dataset (1M rows) where each
dimension attribute had $n$ distinct values with $n$=10\ldots1000. We then
repeated the experiments combining multiple group-bys using these datasets.
Figure \ref{fig:total_time_diff_distinct} shows the results of this test. In the
test dataset, the total number of distinct groups for attributes $a_i$ and $a_j$
is the product of the number of distinct groups for each attribute. We observe
in \ref{fig:total_time_diff_distinct} that the previously-observed minima at
$n_{GB}=2$ is actually a function of the number of distinct groups that are
generated by the multiple-attribute grouping.
Specifically, we observe that the optimal value for the number of distinct
groups is in the range of 10,000 - 100,000. We observe similar trends for temp
table creation and query time.


% \subsubsection{Basic Framework}
% We first examine the performance of \VizRecDB\ without any optimizations.
% For each possible view, we executed the target and comparison view queries
% separately and sequentially, and then picked the top views. This corresponds to
% the basic framework described in Section \ref{sec:basic_framework}. The number
% of queries executed for each dataset was twice the number of views shown in
% Table \ref{tab:datasets}. Figure \ref{fig:baseline_performance} shows the baseline
% performance for Small, Medium and Large datasets of size 1M. We observe that
% execution time increases super-linearly as the size of the dataset (number of dimension and
% measure attributes) increases. Moreover, as mentioned before, even for the
% Medium sized dataset (1M rows, 5 measure and 50 dimension attributes), \VizRecDB\
% execution takes 700s, a latency that is unacceptable for interactive queries.
% 
% \begin{figure}[h]
%   \centering
%     \includegraphics[width=6cm]{Images/baseline_performance.pdf}
%     \caption{Baseline Performance} 
%       \label{fig:baseline_performance}
% \end{figure}

\subsubsection{Combine target and comparison view query}
Next, we study the effect of combining target and comparison view queries as
described in Section \ref{subsec:target_comparison_view}. The goal of this
optimization is to execute both queries in a single scan of the table.
Therefore, the total number of queries executed is equal to the number of
views possible for a given dataset. This optimization offers an average speed up
of 1.7x across a range of selectivities for the input query.

% \subsubsection{Combine Multiple Aggregates}
% \VizRecDB\ uses the parameter $n_{agg}$ to denote the number of aggregates that may
% be included in the same view query. Therefore, given a set of view queries with
% the same group-by attribute, view queries are combined so that each query has up
% to $n_{agg}$ aggregates. We varied $n_{agg} \in {2, 3, 5, 10}$ for each dataset
% (Note that the Small and Medium dataset have only 2 and 5 measure attributes
% respectively).
% Figure \ref{fig:mult_agg} shows the performance gains achieved for the 1M row
% datasets. We see that for a given dataset, increasing $n_{agg}$, i.e. computing
% more aggregates in the same query, gives an almost linear speedup. We also
% notice that this optimization is slightly more effective for larger datasets.
% 
% \begin{figure}[h]
% 
%   \centering
%     \includegraphics[width=6cm]{Images/mult_agg.pdf}
%     \caption{Effect of Multiple Aggregate Optimization} 
%       \label{fig:mult_agg}
% \end{figure}

\subsubsection {Parallel Query Execution}
As discussed in Section \ref{subsec:parallel_exec}, executing view queries in
parallel can provide significant performance gains; however, a high degree of
parallelism can lead to a performance drop off for several reasons. Potential
reasons include disk contention, RAM usage, lock contention, context switches
and cache line contention
\cite{Postgres_wiki}.
Identifying the right amount of parallelism requires tuning for the particular
workload. The \VizRecDB\ workload consists of multiple parallel queries performing
full sequential scans of a given table. To evaluate the
effect of parallelism, we varied the number of queries that can be executed in
parallel and measured its effect on the average time to execute a query as well
as the total execution time. Since our backend DBMS is Postgres, parallel query
execution is implemented by opening multiple connections and running queries
sequentially on each connection.

Figure \ref{fig:parallelism} shows the effect of parallelism on the average
execution time per view (Medium dataset, 1M rows). Note the log scale on the
y-axis.
We observe that query execution time stays flat between 1 - 10 connections, suddenly increases between
10 - 20 connections, and then increases linearly for more than 20 connections.
This suggests that the benefits of parallel execution are outweighed by
contention beyond 20 connections. Figure \ref{fig:parallelism_total} shows the
total time (as opposed to per view execution time) taken by \VizRecDB\ for varying
levels of parallelism. We observe that the minima occurs in the range between 10
- 20 parallel queries and the execution times flatten out after 40 parallel
queries.
This trend is the effect of two opposing factors: (A) increased parallelism
increases contention, and therefore increases per query execution time, and (B)
parallelism decreases the number of batches of queries that must be executed,
thus reducing overall time.
We will take these two opposing forces into account when we develop an
analytical model for \VizRecDB\ execution time in Section \ref{sec:model}.

\begin{figure}[h]
  \centering 
    \includegraphics[width=6cm]{Images/parallelism.pdf}
      \caption{Effect of Parallelism on Per View Execution Time} 
        \label{fig:parallelism}
\end{figure}



\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/parallelism2.pdf}
  \caption{Effect of Parallelism on Total Execution Time} 
    \label{fig:parallelism_total}
\end{figure}




\subsubsection{Analytical Model}
\label{sec:model}
In this section, we use insights from the experimental characterization of
various optimizations to develop an analytical model of \VizRecDB\ performance.
Table \ref{tab:model_params} defines the various parameters used in our model.

\begin{table}
{\center
\vspace{-10pt}
\begin{tabular}[h]{|c|p{5.5cm}|}
\hline
Parameter & Description \\ \hline
$d$ & Number of dimension attributes in table \\
$n$ & Number of rows in input table \\
$n_{tt}$ & Number of rows in temporary table \\
$c_{tt}$ & Number of columns in the temporary table \\
$t_r$ & Time to access a single row of any table \\
$t_c$ & Time to process a single column in a row \\
$t_w$ & Time to write a single row of a table \\
$n_{agg}$ & Number of aggregate attributes computed in a single query \\
$n_{d_{i}}$ & Number of distinct values in attribute $d_i$ \\
$T_{create}$ & Time required to create a temp table \\
$T_{query}$ & Time required to query a temp table \\ 
$b_{create}$ & Number of batches of queries required to create temp tables \\
$b_{query}$ & Number of batches of queries required to query temp tables \\
$n_{conns}$ & Number of queries executing in parallel \\ \hline
\end{tabular}
\vspace{-10pt}
\caption{Analytical model parameters \label{tab:model_params}}
}
\end{table}

% As before, consider the two stages of processing: {\it Temp Table Creation} and
% {\it Temp Table Querying}. In \VizRecDB\, these two stages take place in parallel:
% first, we create a set of temp tables; then we query the temp tables and as we
% finish querying existing tables, we create new tables. The creation of new
% tables and querying of old tables happens in parallel. 

As before, we break our model into two parts, {\it Temp Table Creation} or {\it
Phase 1} and {\it Temp Table Querying} or {\it Phase 2}. Note further that in
each of these phases, multiple queries are executing in parallel. We call the
set of queries executing in parallel as a ``batch'' of queries. Suppose that the
total number of queries to be executed is $q$ and $n_{conn}$ queries can be
executed in parallel. Then the total number of batches required to executed all
the queries is $\frac{q}{n_{conn}}$. We denote the number of query batches used
in temp table creation and querying as $b_{create}$ and $b_{query}$ respectively.

We now describe the analytical model for Phase 1 or {\it Temp Table Creation}.
The time required to create a temp table is proportional to the sum of the time
required to query the input table, aggregate measure attributes and finally
write the temp table. We claim that the time taken to process one row of any
table is equal to the constant time to process any record plus the time required
to process all the columns in the record.
\begin{equation*}
T_{create}\ \ =\ \ n \ast \frac{(t_r\ +\ t_c \ast c)}{n_{conn}} \ +\ t_w
\ast n_{tt} \ast (t_r\ +\ t_c \ast c_{tt})
\label{eq:create_time}
\end{equation*}

We can model {\it Temp Table Querying} similarly. Total time to query a temp
table is the time to query a single row of the temp table multiplied by the
number of rows in the table.
\begin{equation*}
T_{query}\ \ =\ \ n_{tt} \ast (t_r\ +\ t_c \ast c_{tt}) 
\label{eq:query_time}
\end{equation*}
A related quantity we model is $b_{query}$, the number of temp table query
batches (Phase 2 batches) that are issued per temp table creation batch (Phase 1 batch).
If $d_{tt}$ is the number of dimension attributes in a temp table and $n_{agg}$
is the number of measure attributes, the given temp table contains results
for ($d_{tt} \ast n_{agg}$) views, and therefore $d_{tt} \ast n_{agg}$ queries
will be made against the table. Further, since each temp table contains $d_{tt}$
dimension attributes, there will be $Min(\frac{d}{d_{tt}}, n_{conn})$ temp
tables being queried in any batch, where $n_{conn}$ is the number of queries executing
in parallel. Therefore, the total number of queries is $d_{tt} \ast n_{agg}$
$\ast$ $Min(\frac{d}{d_{tt}}, n_{conn})$. Finally, since $n_{conn}$ of these queries
can execute at the same time, $b_{query}$ can be modeled as follows:
\begin{equation*}
b_{query}\ \ =\ \ \frac{d_{tt} \ast nAgg \ast Min(\frac{d}{d_{tt}},
n_{conn})}{n_{conn}}
\end{equation*}

Using the three definitions above, we can model the total time taken by \VizRecDB\
as shown in Equation \ref{eq:total_time}. If there are $b_{create}$ Phase 1
batches, then the total execution time is equal to the time taken to complete
one batch multiplied by $b_{create}$. In turn, the time taken to complete a
Phase 1 batch is the time taken to create temp tables and subsequently query
them in Phase 2 batches.
Since these two steps take place in parallel, we approximate the completion time
for a Phase 1 batch as the difference between the temp table query time and temp
table creation time.
\begin{equation*}
T_{total}\ \ =\ \|(T_{create} - T_{query} \ast b_{query})\| \ast b_{create}
\label{eq:total_time}
\end{equation*}

We evaluate the accuracy of our model by comparing the model predictions to
actual performance results. Figure \ref{fig:create_time_fitted} shows the
accuracy of our model for predicting time to create temporary tables using
Equation \ref{eq:create_time} (Medium dataset, 1M tuples, 5 connections).
Similarly, Figure \ref{fig:query_time_fitted} shows the accuracy of our model
for predicting time to query temporary tables using Equation
\ref{eq:query_time}. Finally, Figure \ref{fig:total_time_fitted} shows the
accuracy of our model for predicting total execution time as derived in Equation
\ref{eq:total_time}.

\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/create_time_fitted.pdf}
  \caption{Actual and Estimated Temp Table Query Time}
  \label{fig:create_time_fitted}
  
\end{figure}
\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/query_time_fitted.pdf}
  \caption{Actual and Estimated Temp Table Query Time} 
  \label{fig:query_time_fitted}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/total_time_fitted.pdf}
  \caption{Actual and Estimated Total Execution Time} 
    \label{fig:total_time_fitted}
\end{figure}

\subsection{Choosing \VizRecDB\ parameters based on the model}

{\bf Choosing Number of Parallel Queries:} The speed up offered by running
queries in parallel depends on the DBMS parameters such as maximum number of
connections, shared buffer size, etc. In our implementation, each phase of
processing can issue up to $n_{conn}$ queries in parallel, so in principle,
there may be up to $2 \ast n_{conn}$ queries running in parallel. Since each
DBMS has a maximum number of queries that can be executed in parallel,
$n_{conn}$ must be set to be smaller than half the maximum number of
connections. The exact number of connections will depend on the other workload
in the system and the size of the dataset. In our setup, $n_{conns}$=40 gives
the best results for a range of dataset sizes and system parameters.

{\bf Choosing $n_{agg}$:} Combining multiple aggregates offers a performance
gain that is almost linear in the number of aggregates, without any significant
penalty. As a result, we set $n_{agg}$ equal to the number of measure attributes
in the table.

{\bf Choosing dimension attributes to combined processing:}  As discussed in
Section \ref{subsec:mult_gb_expt}, the optimal number of distinct groups
consistently falls in the range 10,000 to 100,000. As a result, we set
$n_{groups}$, the maximum number of groups that any query can generate, to
100,000.
This parameter is used to pick dimension attributes that will be combined into a
single view query. For a set of attributes $a_1$\ldots$a_n$, the maximum number
of distinct groups that can be generated is $\prod_i a_i$. This is the worst
case bound since correlation between two attributes can only decrease the number
of distinct groups. \VizRecDB\ models the problem of grouping attributes with a
constraint on number of groups as a variant of bin-packing.
Specifically, \VizRecDB\ adopts the following problem formulation.

Let $n_{d_{i}}$ be the number of distinct values for dimension attribute $d_i$.
The traditional definition for bin packing states that given bins of size $V$
and $n$ items with size $a_1$\ldots$a_n$, find the minimum integer number of
bins and a corresponding partition of the set of items such that $\sum_{j} a_j$
$\leq$ $V$ for all $a_j$ belong to any given partition. In our setting, there is
a limit on the {\it product} (as opposed to the sum) of the sizes of items. As a
result, we formulate the problem as follows: Given bins (queries) of size $V$
($V$=100,000) and $d$ attributes with sizes $log(n_{d_{i}})$, find
the minimum number of bins and a corresponding partition of the $d$ attributes
such that $\sum_{j} log(n_{d_{j}})$, i.e. $\prod_{j} n_{d_{i}}$, $\leq$ $V$.
Bin-packing has been well studied and we use off-the-shelf software
\cite{glpk} to perform bin-packing on the dimension attributes.

\subsubsection{Experimental Evaluation with All Optimizations}

We now show performance results for \VizRecDB\ using the optimal parameter settings
described above. Specifically, we set $n_{conns}$=40, $n_{agg}$=$m$ (i.e.
number of measure attributes) and $n_{groups}$=100,000 for bin-packing. Further,
we apply the optimization of combinging target and comparison queries for each view. From
Figure \ref{fig:total_speed_up}, we see that the combination of all
optimizations gives us a speedup of about 100X for the Medium and Large
datasets (note the log scale on Y axis). Although the impact of optimizations is
relatively small for the Small dataset, we still observe that the total
execution time is halved. We thus see that the all the optimizations
taken together can enable \VizRecDB\ to run at near-interactive response times.

\begin{figure}[h]
  \centering
    \includegraphics[width=6cm]{Images/total_speedup.pdf}
  \caption{Performance Speedup With Optimizations} 
  \label{fig:total_speed_up}
\end{figure}

\subsection{How long does \VizRecDB\ take to find interesting views?}

In this section, we use knowledge about optimal parameters to evaluate the
performance of \VizRecDB\ on a suite of different datasets.

\subsection{End-to-End Testing with Real Datasets}
