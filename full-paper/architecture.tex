%!TEX root=document.tex

\section{{\large \SeeDB\ } Architecture}
\label{sec:system_architecture}

%In this section, we present an overview of the \SeeDB\ architecture.
% Given an input query on a dataset, the goal of \SeeDB is to recommend
% visualizations the analyst would find most valuable.
% \SeeDB recommends visualizations that the analyst would find most valuable,
% taking into account aspects such as the background, user preferences,
% and context (as discussed in Section~\ref{sec:general-metric}).
\agp{First two sentences seem disconnected. Might be good to talk about two separate aspects in two paragraphs: front-end and user experience, and backend arch.}
\SeeDB is implemented as a middleware layer that can run on
top of any SQL-compliant database system. 
We packaged \SeeDB as a recommendation plugin that can
be incorporated into a visualization tool such as Tableau or Spotfire. 
% As such, \SeeDB\ would ideally be used as a plug-in for existing 
% visualization software.
For evaluating our system, we have combined \SeeDB with a simple visualization
builder interface inspired by Polaris~\cite{polaris}.
Figure \ref{fig:frontend} shows the web frontend for \SeeDB which is composed
of four parts:
(A) dataset selector used to
connect to a database and query a specific table; (B) query builder used to
formulate queries using a form-based interface; (C) visualization builder
used to manually specify visualizations; and (D) \SeeDB recommendations plugin
that displays recommended visualizations.
Recommendations provided by \SeeDB change in response to changes in the query (B)
issued against the database.
\srm{This is the first mention of ``human in the loop'' philiosophy.  I think it might be good
to promote this to an earlier part of the paper, but only if we feel like we have something interesting
to say about it.}
We note that the {\em mixed-initiative} design of \SeeDB incorporating both manual
visualization specification and automated recommendations, reflects the
human-in-the-loop philosophy underlying \SeeDB.
We seek to speed up the analytics process not by completely automating the task, but
by providing the analyst real-time support while letting him or her drive the process.
% We seek to support the analytics process while letting the analyst be in the 
% driver's seat. \mpv{Fix sentence.}

\begin{figure}[htb]
\vspace{-10pt}
\centerline{
\hbox{\resizebox{7cm}{!}{\includegraphics[trim=20mm 45mm 30mm 55mm, 
clip=true]{Images/seedb-architecture.pdf}}}}
\vspace{-18pt}
\caption{SeeDB Architecture}
\label{fig:sys-arch}
\vspace{-12pt}
\end{figure} 
\mpv{Update arch diagram}

Figure \ref{fig:sys-arch} depicts the overall architecture of our
system.  The \SeeDB client is a web-based front-end that captures user
input and renders visualizations produced by the \SeeDB server.  The
\SeeDB server is composed of four main components.  The first
component, the {\it view generator}, is responsible for parsing the
input query, querying system metadata and generating the list of
visualization queries that must be evaluated.  


\agp{Previous:}
The engine executes
queries using the underlying DBMS, but applies two types of
optimization in this process.  The first set of optimizations involve
{\it batching}, where DBMS queries are combined to reduce the number
of distinct SQL statements and scans of data that must be sent
executied by the DBMS (Section \ref{}).  The second set of
optimizations involves discarding low-utility views using pruning
(Section \ref{}).  Rather than trying to estimate the quality of a
particular view without looking at the data, the pruner runs in a {\it
  phased} manner, evaluating batched queries on a small subset of the
data, using results on that subset to estimate the quality of each
view, and discarding views with low estimated utility.  The retained
views are then run on another subset of the data in the next phase of 
execution.


\agp{Intended:
The goal of the engine is to evaluate a collection of queries
using the underlying DBMS, applying two kinds of optimizations:
{\em sharing}, where aggregate view queries are combined to share computation
as much as possible, and {\em pruning}, where aggregate view queries
corresponding to low utility visualizations are dropped from consideration.
These optimizations are largely orthogonal to each other.
}

\agp{
To derive benefits from both these kinds of optimizations,
we develop a {\em phase-based execution framework}, that proceeds in {\em phases},
each phase operating on a small subset of data.
For instance, phase $i$ of $n$ examines the $i$th fraction of the dataset
if divided into $n$ parts.
We start with the entire set of views under consideration.
\begin{denselist}
\item Within each phase, we only apply {\em sharing-based optimizations}
to share computation as much as possible with each other among the set
of views still under consideration.
\item At the end of each phase, we only apply {\em pruning-based optimizations}
to determine which views to discard.
\end{denselist}
In this manner, the set of views under consideration
decreases across these phases, with all views at the start 
of the first phase, and only the $k$ views left
at the end of the $n$ th phase.
Further, in this manner, the sharing and pruning based optimizations do
not interfere with each other -- one is applied during the phase,
and one is applied at the end of the phase.
}

\agp{
	In the next section, we describe the \SeeDB optimizer,
	specifically, the sharing based optimizations in Section 4.1 (Section~\ref{sec:sharing_opt}) and the pruning based optimizations in Section 4.2 (Section~\ref{sec:in_memory_execution_engine}). --- Manasi -- I left the numbers in there because the refs should correspond to those two -- I've used those refs in the intro.
}

\reviewer {
	D2.5 The horizontal partitioning is never detailed: how is it done? How is the
number of fragments decided?
}
\mpv{something about how partitions are defined}

Once \SeeDB has identified the top views, the results are returned to the frontend 
 \SeeDB which generates and displays the recommended visualizations.

\mpv{I would like to keep the pseudocode and cut other stuff}

% The optimizer applies multi-query
% optimization 
%  which is responsible for applying multi-query 
% optimization (Section \ref{}) to combine and rewrite DBMS queries 
% corresponding to view in running.
% Next, the execution engine issues optimized queries to the underlying DBMS
% and performs post-processing to compute results for individual views.
% Results from the execution engine are then fed to the pruner which leverages
% view pruning techniques to discard low-utility views.
% The set of views that pass muster with the pruner re-enter the optimizer for
% the next phase of execution.
% Once \SeeDB has identified the top views, the results are returned to the client 
% where \SeeDB frontend generates and displays the recommended visualizations.

% the view generator module queries the system metadata for table sizes, 
% column types, and correlations between column values. 
% It then uses this metadata and the incoming query to remove redundant aggregate views and generate ``stubs'' for the remaining aggregate views.
% The view stubs are then passed to the execution engine which is responsible 
% executing the aggregate views and performing run-time view pruning to identify
% high-utility views.
% To execute aggregate views, the execution engine optimizes the set of aggregate
% views, turns view stubs into SQL queries, and uses the underlying DBMS to efficiently execute the aggregate queries. \mpv{Fix sentence}.
% Once the \SeeDB server has executed the aggregate views and identified the top-k
% views, the data underlying top views is sent to the client where \SeeDB frontend 
% generates and displays the recommended visualizations.

% However, in this work, for ease of development and testing, we built 
% \SeeDB as a standalone end-to-end system that allows users to pose 
% arbitrary queries over data and obtain recommended visualizations.

% The standalone version of \SeeDB\ is comprised of two main components: 
% a light-weight client that is 
% used to issue queries, display recommended visualizations, and allow basic 
% interactions with visualizations; 
% and a server used to generate candidate
% aggregate views, execute
% them over data and identify the aggregate views with highest utility. 
% Figure \ref{fig:sys-arch} depicts the architecture of our system.
% Once the analyst chooses the data of interest (by issuing a query $Q$), the
% client makes a call to the backend for visualization recommendations for $Q$.
% Aggregate view stubs are essentially more elaborate triples of the form $(a, m, f)$ as discussed in Section \ref{sec:problem_statement}. 


% The generated aggregate view stubs are then sent to the execution engine
% responsible for querying the underlying data, evaluating the utility of each
% candidate view, and identifying the top views of interest. 

% Figure \ref{fig:frontend1} shows the \SeeDB client in action; including the supported mechanisms to specify an input query 
% and the visualizations generated by a sample query. 

% In the backend, the view generator module queries the system metadata for table sizes, 
% column types, and correlations between column values. 
% It then uses this metadata and the incoming query to remove redundant aggregate views and generate ``stubs'' for the remaining aggregate views. 
% Aggregate view stubs are essentially more elaborate triples of the form $(a, m, f)$ as
% discussed in Section \ref{sec:problem_statement}. 
% The generated aggregate view stubs are then sent to the execution engine
% responsible for querying the underlying data, evaluating the utility of each
% candidate view, and identifying the top views of interest. 
% Once the \SeeDB\ backend has identified the best aggregate views, the \SeeDB\
% client generates and displays recommended visualizations based on these views.
% Figure \ref{fig:frontend1} shows the \SeeDB client in action; including the supported mechanisms to specify an input query 
% and the visualizations generated by a sample query. 

We begin with a discussion of the optimizer and the set of multi-query optimization 
techniques used by \SeeDB.


% In this paper, we build the execution engine in three stages. 
% In the first stage \mpv{better word?}, we implement \SeeDB as a layer on top of the database system and apply traditional multi-query optimization 
% techniques to see how far we can push existing relational databases to support a \SeeDB-type workload.
% While we obtain resonable performance for small datasets, we find the optimizations are insufficient to support large datasets \mpv{quantify?}.
% Therefore, next, we develop a set of pruning techniques that can use running estimates of utility to rapidly prune low-utility views. 
% By reducing the number of views evaluated, these techniques reduce overall latency and also allow \SeeDB to return results without processing the entire dataset.
% We implement these pruning strategies in a simple shared scan system to test the efficacy of our techniques.
% Finally, we build a hybrid execution engine that combines our pruning strategies with the performance optimizations of the DBMS-backed execution engine,
% thus giving us the best of both worlds. 

% In this paper, we explore two distinct implementations of the \SeeDB\ execution
% engine. 
% Our first implementation draws upon traditional multi-query optimization~\cite{}
% and OLAP literature~\cite{} to implement \SeeDB\ as a wrapper
% on top of a database system. \mpv{These techniques are usually inside the DBMS, 
% we are using them outside?}
% The goal of this implementation is to study how far 

% While we obtain reasonable performance by employing well-known optimizations, we find that 
% operating through the SQL interface limits the performance we can obtain.
% Therefore, we next built a custom execution engine that completely shares query scans between
% all views and uses heuristics to rapidly prune low-utility views.
% These techniques allow us to perform only a single pass over the data
% and rapidly identify top visualizations. \mpv{Incorp into DB?}
% However, existing systems do not provide a good means to share scans between
% queries or to access intermediate results during scans.
% As a result, optimization opportunities are limited.
% To overcome the constraints of existing database systems, we implement a
% simple, custom Execution Engine for \SeeDB\ optimized to share scans
% across all views and perform pruning based on intermediate results. 
% In an ideal solution, shared scans and pruning would be implemented inside the
% database; however, for the purpose of this work, we implement the \SeeDB\
% execution engine as a standalone proof-of-concept with a brief discussion of how
% the \SeeDB\ engine could be made part of a DBMS. \mpv{need to add this discussion 
% somewhere}

% Next we briefly examine the \SeeDB client and then describe the execution engines
% in detail.

% In the DBMS-based execution engine (Section \ref{}), the view stubs are passed
% through the optimizer that identifies the best ways to combine the queries to minimize
% execution time.
% Once the views have been optimized, the views are rewritten as SQL queries and
% executed against the underlying database. 
% The results of these queries are
% processed to update the view stubs and compute view utility. 
% Once all the queries have been processed, the top-k views are returned to the
% frontend.
% 
% In the main-memory execution engine, \SeeDB\ makes a single pass through the
% data (either read from disk or already present in memory) and keeps running
% estimates of utility for each of the views stubs obtained from the View
% Generator. 
% As the engine processes more of the data, the utility estimates become more
% accurate and \SeeDB\ uses various pruning heuristics to prune out views on the fly.
% By the time the full data has been processed, \SeeDB\ has identified the top-k
% views with the largest utility that are then returned to the frontend.


