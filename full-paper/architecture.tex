%!TEX root=document.tex


\section{{\large \SeeDB\ } Design Overview}
\label{sec:system_architecture}

In this section, we present an overview of the \SeeDB\ architecture.
Given an input query on a dataset, 
\SeeDB recommends visualizations that the analyst would find most valuable,
taking into account aspects such as the background, user preferences,
and context (as discussed in Section~\ref{sec:general-metric}).
Conceptually, \SeeDB\ is designed as a recommendation plugin that can
be applied to any visualization engine,
including Tableau or Spotfire. 
% As such, \SeeDB\ would ideally be used as a plug-in for existing 
% visualization software.
However, in this work, for ease of development and testing, we built 
\SeeDB\ as a standalone end-to-end system that allows users to pose 
arbitrary queries over data and obtain recommended visualizations.

The standalone version of \SeeDB\ is comprised of two main components: 
a light-weight client that is 
used to issue queries, display recommended visualizations, and allow basic 
interactions with visualizations; and a server used to generate candidate
aggregate views, execute
them over data and identify the aggregate views with highest utility. 
Figure \ref{fig:sys-arch} depicts the architecture of our system.
Once the analyst chooses the data of interest (by issuing a query $Q$), the
client makes a call to the backend for visualization recommendations for $Q$.
At the backend, the view generator module queries the system metadata for table sizes, 
column types, and correlations between column values. 
It then uses this metadata and the incoming query to remove redundant aggregate views and generate ``stubs'' for the remaining aggregate views. 
Aggregate view stubs are essentially more elaborate triples of the form $(a, m, f)$ as
discussed in Section \ref{sec:problem_statement}. 
The generated aggregate view stubs are then sent to the execution engine
responsible for querying the underlying data, evaluating the utility of each
candidate view, and identifying the top views of interest. 
Once the \SeeDB\ backend has identified the best aggregate views, the \SeeDB\
client generates and displays recommended visualizations based on these views.
Figure \ref{fig:frontend1} shows the \SeeDB client in action; including the supported mechanisms to specify an input query 
and the visualizations generated by a sample query. 

\begin{figure}[htb]
\vspace{-10pt}
\centerline{
\hbox{\resizebox{7cm}{!}{\includegraphics[trim=10mm 45mm 55mm 60mm, 
clip=true]{Images/SeeDB-architecture.pdf}}}}
\vspace{-18pt}
\caption{SeeDB Architecture}
\label{fig:sys-arch}
\vspace{-12pt}
\end{figure} 

We implement \SeeDB's execution engine as a layer on top of a
traditional relational database system.
This enables us to seamlessly reuse all the benefits
of traditional relational databases --- the ability
to pose declarative queries, query optimization, transactional guarantees,
and so on.
That said, naively using a traditional database system
would lead to severe performance issues.
Our core focus in the subsequent sections will
be to propose a collection of optimization schemes
that will enable \SeeDB to be much more performant 
on large datasets.


% In this paper, we build the execution engine in three stages. 
% In the first stage \mpv{better word?}, we implement \SeeDB as a layer on top of the database system and apply traditional multi-query optimization 
% techniques to see how far we can push existing relational databases to support a \SeeDB-type workload.
% While we obtain resonable performance for small datasets, we find the optimizations are insufficient to support large datasets \mpv{quantify?}.
% Therefore, next, we develop a set of pruning techniques that can use running estimates of utility to rapidly prune low-utility views. 
% By reducing the number of views evaluated, these techniques reduce overall latency and also allow \SeeDB to return results without processing the entire dataset.
% We implement these pruning strategies in a simple shared scan system to test the efficacy of our techniques.
% Finally, we build a hybrid execution engine that combines our pruning strategies with the performance optimizations of the DBMS-backed execution engine,
% thus giving us the best of both worlds. 

% In this paper, we explore two distinct implementations of the \SeeDB\ execution
% engine. 
% Our first implementation draws upon traditional multi-query optimization~\cite{}
% and OLAP literature~\cite{} to implement \SeeDB\ as a wrapper
% on top of a database system. \mpv{These techniques are usually inside the DBMS, 
% we are using them outside?}
% The goal of this implementation is to study how far 

% While we obtain reasonable performance by employing well-known optimizations, we find that 
% operating through the SQL interface limits the performance we can obtain.
% Therefore, we next built a custom execution engine that completely shares query scans between
% all views and uses heuristics to rapidly prune low-utility views.
% These techniques allow us to perform only a single pass over the data
% and rapidly identify top visualizations. \mpv{Incorp into DB?}
% However, existing systems do not provide a good means to share scans between
% queries or to access intermediate results during scans.
% As a result, optimization opportunities are limited.
% To overcome the constraints of existing database systems, we implement a
% simple, custom Execution Engine for \SeeDB\ optimized to share scans
% across all views and perform pruning based on intermediate results. 
% In an ideal solution, shared scans and pruning would be implemented inside the
% database; however, for the purpose of this work, we implement the \SeeDB\
% execution engine as a standalone proof-of-concept with a brief discussion of how
% the \SeeDB\ engine could be made part of a DBMS. \mpv{need to add this discussion 
% somewhere}

% Next we briefly examine the \SeeDB client and then describe the execution engines
% in detail.

% In the DBMS-based execution engine (Section \ref{}), the view stubs are passed
% through the optimizer that identifies the best ways to combine the queries to minimize
% execution time.
% Once the views have been optimized, the views are rewritten as SQL queries and
% executed against the underlying database. 
% The results of these queries are
% processed to update the view stubs and compute view utility. 
% Once all the queries have been processed, the top-k views are returned to the
% frontend.
% 
% In the main-memory execution engine, \SeeDB\ makes a single pass through the
% data (either read from disk or already present in memory) and keeps running
% estimates of utility for each of the views stubs obtained from the View
% Generator. 
% As the engine processes more of the data, the utility estimates become more
% accurate and \SeeDB\ uses various pruning heuristics to prune out views on the fly.
% By the time the full data has been processed, \SeeDB\ has identified the top-k
% views with the largest utility that are then returned to the frontend.


