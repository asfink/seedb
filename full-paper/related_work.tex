%!TEX root=document.tex


\section{Related Work}
\label{sec:related_work}
\SeeDB\ is draws on related work from multiple areas;
we review papers in each of the areas, and describe how they relate to
\SeeDB. 

\reviewer {
	4.1 I do not agree with the comment that the work of reference [31] is
independent of queries. In [31] a user query (or more precisely, knowing
what part of the cube has already been explored) is needed
}

\resolved{\reviewer {
	4.2 As the SeeDB approach is basically to suggest queries based on the
current user's query, it would be interesting to position (and compare) SeeDB
among the approaches of query recommendation in databases, especially one
of the authors' previous works (QueRIE: Collaborative Database Exploration.
TKDE 26(7): 17781790 (2014))
}}

\stitle{Visualization Tools}:
The visualization 
research community has introduced a number of
visual analytics tools such as ShowMe, Polaris, Spotfire and 
Tableau~\cite{DBLP:journals/cacm/StolteTH08, DBLP:journals/tvcg/MackinlayHS07, Ahlberg:1996:SIE:245882.245893}.
These tools do provide some features for automatically selecting
the best visualization for a data set, 
% In these tools, the goal is to make it simpler for analysts 
% to pose specifications of what they would like to examine
% (e.g., they would like to examine sales by year for cars),
% and the visual analytics tool would provide an appropriate
% visualization medium for this visualization specification
% (e.g., a trend line or a bar chart).
% Most of the ``smarts'' in these tools is restricted
%to a set of 
but these features are restricted to a set of aesthetic rules of thumb that
guide which visualization is most appropriate.
%given a visual specification.
Similar visual specification tools 
have also been introduced by the
database community, including Fusion
Tables~\cite{DBLP:conf/sigmod/GonzalezHJLMSSG10} and the
Devise~\cite{DBLP:conf/sigmod/LivnyRBCDLMW97} toolkit. 
In all these tools, the user must choose the data they want to visualize, requiring
a tedious iteration through all sets of attributes.
\techreport{
For datasets with a large number of attributes,
it is often hard for the analyst to manually study
every single attribute.
}
In contrast, in \SeeDB, our goal is to automatically recommend visualizations
based on a generalized distance function, finding
attribute sets that maximize the value of this function.
%in addition to the simple visualization
%specification functionalities described above,
%which is more appropriate when the analyst already
%knows which visualizations they should be examining.

% SRM -- I don't think anyone is going to say that Matlab does what we do
%Statistical analysis and graphing packages such as R, SAS and Matlab could also
%be used generate visualizations, but they lack the ability to filter and
%recommend visualizations. 


\stitle{Partial Automated Selection of Visualizations.}
%A few systems have attempted to automate some aspects of data analysis
%and visualization. For example, 
Profiler 
detects anomalies in data \cite{DBLP:conf/avi/KandelPPHH12} and provides
some visualization recommendation functionality,
but is restricted determining the best binning for the
for the $x$ axis: in particular, it decides which granularity
is appropriate to bin on to depict the most interesting relationships 
between data. 
Since this is a much simpler (and less ambitious) problem
than ours, sophisticated techniques are not necessary.
% Our work is also similar to VizDeck which is a tool that given a dataset, uses a
% set of pre-determined rules to create diverse visualizations and
% allows the user to pick and choose the visualizations that seem relevant
% \cite{DBLP:conf/sigmod/KeyHPA12}.
% Thus, while powerful, VizDeck requires much more manual input than \SeeDB. 
% In addition, the visualizations generated by VizDeck do not leverage the
% context of the underlying dataset, making the visualizations generated by
% both systems very different in flavor. 
% It would be instructive to augment
% VizDeck visualizations with \SeeDB\ visualizations to study their relative
% utility.
%Another related tool 
%is 
VizDeck~\cite{DBLP:conf/sigmod/KeyHPA12} is a tool that, given a dataset,
depicts all possible 2-D visualizations on a dashboard.
% that the user can
%manipulate by reordering or pinning visualizations.
Given that VizDeck generates all visualizations, it is only meant for 
small datasets; additionally, the VizDeck does not discuss techniques
to speed-up the generation of these visualizations. 


\stitle{Scalable Visualizations.}  There has been some recent work on
scalable visualizations that employ in-memory caching, sampling, and
pre-fetching to improve the interactivity of visualization systems
backed by
databases~\cite{2013-immens,DBLP:conf/avi/KandelPPHH12,hotmap,doshi2003prefetching,DBLP:journals/corr/KimBPIMR14}.
Such techniques could be employed in our settings to further improve
response times (although some of these techniques, such as in-memory caching,
can only work with small datasets.)


% . Immens~\cite{2013-immens} and Profiler (mentioned above)
% maintain a data cube in memory and use it to support rapid user
% interactions. While this approach is possible when the dimensionality
% and cardinality is small (e.g., simple map visualizations of a single
% attribute) it cannot be used with large tables and ad-hoc queries.
% Pre-computation and pre-fetching are two other techniques that have
% been used for scalability, e.g., \cite{hotmap} uses precomputed image
% tiles for geographic visualization, \cite{doshi2003prefetching} uses
% extensive pre-fetching and caching.  Our recent paper describes
% techniques for generating visualizations approximately, but with
% ordering guarantees~\cite{DBLP:journals/corr/KimBPIMR14}; these
% techniques could be easily employed in our setting to further improve
% the response times.  
%A recent paper \cite{2014-viz-latency} discusses
%how high latency in visualization systems reduces the rate at which
%users observe, analyze and draw conclusions from data, thus making a
%strong case for interactive response times.

%Finally, finding interesting visualizations in data also involves understanding
%user preferences. 
%In future work, we plan to learn user preference models towards visualizations
%using techniques similar to \cite{CHI:YangLZ14, IUIGotzW09}. 


% \agpneutral{Other recent work has addressed other aspects of visualization
% scalability, including prefetching and caching~\cite{doshi2003prefetching}, data
% reduction~\cite{burtini2013time} leveraging time series data
% mining~\cite{esling2012time}, clustering and
% sorting~\cite{guo2003coordinating,seo2005rank}, and dimension
% reduction~\cite{Yang:2003:VHD:769922.769924}. These techniques are orthogonal to
% our work, which focuses on speeding up the computation of a single visualization
% online.}\\

\stitle{Data Cube Materialization:} 
%OLAP (Online Analytical Processing)~\cite{olap} 
%concerns itself with the 
Computations on {\em data cubes}~\cite{DBLP:jounral/DMKD/GrayCBLR97}
involve aggregating across multiple dimensions.
%,
%considering all possible groupings of attributes,
%at different granularities, and all possible aggregations of other attributes.
%These data cubes are used for report generation for Business Intelligence
%(BI) applications.
Even when the number of attributes and number
of distinct values of each attribute is relatively small,
the space required to materialize then entire cube can be prohibitive,
meaning that only a few (if any) dimensions can be pre-aggregated.
%and store these data cubes is
%very large --- as a result of this, in practice, data cubes
%are only stored, if at all, for a small number of attributes 
%(typically less than a handful).
There has been some work on identifying, given a query workload,
which cubes to materialize within a certain storage budget,
so as to minimize the amount of work to be performed when a
query is provided~\cite{DBLP:conf/VLDB/AgarwalADG96,DBLP:conf/SIGMOD/HarinarayanRU96}.
%In our setting, since the number of attributes can be rather large,
%and since any ad-hoc data exploration queries can be provided on-the-fly, materialization
%is not an option except for the most common or popular attributes.
Hence, the optimization techniques underlying cube materialization 
are similar in spirit to our batching optimizations in Section~\ref{sec:sharing_opt},
however, they focus on offline computation of views to minimize storage rather than efficient online
optimization.

% (but not similar to our pruning optimizations).
% However, the underlying techniques are different from ours in two ways:
% (1) Cube materialization computes
%  these aggregates offline, as opposed to online: the bottleneck
% is not computation, but storage.
% (2) We must compute the results for
% all possible views, cube materialization only seeks 
% to compute a few (good) sub-cubes. Our algorithms focus on
% grouping views into batches,
% each of which fit within our resource constraints. 
% In cube materialization there is a single ``batch'' of aggregates
% that is computed offline (rather than multiple batches) and then stored.


\stitle{Browsing Data Cubes:}
There has been some  work on using data mining techniques
to aid in the exploration of data 
cubes~\cite{DBLP:conf/vldb/Sarawagi99, DBLP:conf/vldb/SatheS01, DBLP:conf/vldb/Sarawagi00, 
DBLP:conf/SIGKDD/OrdonezC09}.
Sarawagi et al.~\cite{DBLP:conf/EDBT/SarawagiAM98, DBLP:conf/vldb/Sarawagi00} 
 explored 
the question of finding ``interesting'' cells in a cube.
The interestingness of a cell is defined by how surprising
its value is given the other values in the cube:
\cite{DBLP:conf/EDBT/SarawagiAM98} uses techniques 
based on a table analysis method while
\cite{DBLP:conf/vldb/Sarawagi00} uses techniques based on entropy to find interesting cells.
These techniques generally identify sub-cubes of a cube that produce the most
deviation amongst all sub-cells, analogous to SeeDB finding the (single) dimension attribute that shows the greatest variation in a given
aggregation query.  In contrast, SeeDB focuses on
 finding variation vis-a-vis a reference data set, recommending multiple views over a large set of possible visualizations.

%Inherent interestingness is a much simpler problem than the
%one addressed in our paper, since it can be precomputed before
%any queries arrive. (This is because inherent interestingness is independent of queries.) 
%Due to this, 
%the techniques in browsing \cite{DBLP:conf/EDBT/SarawagiAM98, 
%DBLP:conf/vldb/Sarawagi00} cannot be applied in our context.
%Furthermore, instead of comparing individual cells, 
%\SeeDB\ evaluates sets of aggregates (i.e. distributions
%for our view), thus focusing on trends in values rather than individual values:
%unlike individual cube values, trends can be more easily displayed
%and examined on visual interfaces.


In \cite{DBLP:conf/vldb/Sarawagi99}, Sarawagi
proposes techniques to explain an
increase or decrease in a specific aggregate by drilling down into that aggregate.
In constrast, \SeeDB\ seeks to find interesting
differences between two datasets that have not yet been aggregated along any dimensions.
%,
% rather than explaining the cause of variations observed in already aggregated
%quantities.  
Wu et al~\cite{scorpion} tackle a similar problem in Scorpion,
and differ for similar reasons.
%\agp{The prev paragraph is quite weak. I'll need to look into the paper to identify the 
%differences.}

% Second, instead of finding ``intrinsically'' interesting aggregates, 
% \SeeDB\ evaluates aggregate views with respect to a comparison dataset. 

% \agp{If we do have a general metric, get rid of sentence below.}
% We note however that incorporating an ``intrinsic'' interesting-ness metric into our utility function is an avenue for future work.


\stitle{Multi-Query Optimization:} Our batching optimizations
%presented in Section~\ref{sec:sharing_opt} 
draw on
related techniques from literature on shared
scans~\cite{Fernandez:1994:RBW:191843.191947} and multi-query
optimization
~\cite{DBLP:journals/tods/Sellis88,DBLP:journals/pvldb/KementsietsidisNCV08,DBLP:journals/pvldb/WangC13}.
%{\em shared scans}~\cite{DBLP:conf/VLDB/ZukowskiHNB07,DBLP:conf/sigmod/HollowayRSD07,DBLP:journals/pvldb/UnterbrunnerGAFK09,DBLP:conf/icde/RamanSQRDKNS08,DBLP:journals/pvldb/QiaoRRHL08}
% Our goal of evaluating
%many views in parallel is an instance of multi-query 
%optimization,
%i.e., optimizing the execution of a collection of queries rather
%than one at a time.
%Multi-query optimization focuses on the creation of ``global''
%query plans for a collection of queries rather than a single one,
%such that the intermediate operators and results are shared across queries.
Our problem is simpler however, since we know which candidate
visualizations we need to evaluate, given a query, rather than having
to wait for a batch of queries to arrive.  Hence, we can choose how to
batch aggregates.  Further, because we are only executing simple
aggregation queries with predictable selectivities, we can deploy
optimizations tailored to our setting.  Finally, our pruning
techniques allow us to stop evaluating some visualizations if we find
that their utility is low---other multi-query schemes do not have the
luxury of not performing particular computations.

\stitle{Query Recommendation Systems}: Finally, there is some related work in
the space of general query recommendation in databases (see
~\cite{marcel2011survey} for an overview.)  These systems are
generally designed to help users pose users relevant
queries over of a database, typically by consulting historical query
workloads and using statistical similarity or recommender algorithms
to refine user inputs.  These techniques, to the best of our
knowledge, focus on recommending SQL queries instead of visualizations
(and hence don't focus on visually relevant distance metrics.)  However, we
believe they could be integrated into a generalized utility metric
inside SeeDB, although a full user-study comparing their effectiveness
is outside the scope of this work.





% The work done in \SeeDB\ is similar to previous literature in
% browsing OLAP data cubes. 
% Instead of building complete data cubes,
% one can think of \SeeDB\ views as projections of the cube along various
% dimensions.
%  Data cubes have been very well studied in the literature
% \cite{DBLP:conf/SIGMOD/HarinarayanRU96, DBLP:jounral/DMKD/GrayCBLR97}, and work such as
% ~\cite{DBLP:conf/vldb/Sarawagi99, DBLP:conf/vldb/SatheS01,
% DBLP:conf/vldb/Sarawagi00, DBLP:conf/SIGKDD/OrdonezC09} has explored the
% questions of allowing analysts to find explanations for trends, get suggest for
% cubes to visit, identify generalizations or patterns starting from a single
% cube. 
% This literature is not directly applicable to our problem since the cubes we
% are considering have 10s to 100s of dimensions, making traditional cube
% algorithms infeasible. 


% \stitle{General Purpose Data Analysis Tools:}
% Our work is also related to data mining and the work on building general purpose
% data analysis tools on top of databases. 
% For example, MADLib \cite{DBLP:conf/VLDB/HellersteinRSWF12}
% implements various analytic functions inside the database. 
% MLBase similarly
% \cite{DBLP:conf/CIDR/KraskaTDGFJ2013} provides a platform that allows users to
% run various machine learning algorithms on top of the Spark system
% \cite{DBLP:conf/SCC/ZahariaCFSS10}.


\techreport{
\stitle{Other Related Work:}
The techniques we use in our custom implementation of \SeeDB\ is
related to work in top-k ranking and multi-armed bandits.
The confidence interval-based technique discussed in Section \ref{sec:confidence_interval} 
is similar to top-k based pruning algorithms developed 
in other contexts~\cite{DBLP:conf/pods/FaginLN01, 
DBLP:conf/vldb/IlyasAE04, DBLP:conf/ICDE/ReDS07}.
%and similar to our work on  
%sampling for visualizations~\cite{DBLP:journals/corr/KimBPIMR14}.
The techniques in these papers on top-$k$ pruning are 
tailored more towards picking individual desirable tuples,
rather than visualizations (which correspond to an array of aggregates),
and thus the techniques are very different.
%The emphasis in the latter work is to ensure the generation of
%approximate visualizations with certain guarantees---here,
%our goal is to evaluate visualizations (possibly approximately). 
Similarly, multi-armed bandits (see Section \ref{sec:multi_armed_bandit}) is another
related area of research. 
Our problem presents a novel application of multi-armed bandit strategies to 
exploratory data analysis.
The technique we adopt is from a recent paper on top-$k$ MAB~\cite{BubeckWV13,audibert2010best} and is a 
a variant of the original UCB algorithm \cite{AuerCF02, LaiR85}.}
