%!TEX root=document.tex


\section{Related Work}
\label{sec:related_work}
\SeeDB\ is related to work from multiple areas;
we review papers in each of the areas, and how they relate to
\SeeDB. 

\stitle{Interactive Data Visualization Tools}:
Over the past few years, the research community has introduced a number of
interactive data analytics tools such as ShowMe, Polaris, and 
Tableau~\cite{DBLP:journals/cacm/StolteTH08, DBLP:journals/tvcg/MackinlayHS07}.
Similar visualization specification tools have also been introduced by the
database community, including Fusion
Tables~\cite{DBLP:conf/sigmod/GonzalezHJLMSSG10} and the
Devise~\cite{DBLP:conf/sigmod/LivnyRBCDLMW97} toolkit. 
Unlike \SeeDB, which recommends visualizations automatically, these tools place
the onus on the analyst to specify the visualization to be generated.
For datasets with a large number of attributes, it is not possible
for the analyst to manually study all the attributes; hence, interactive
visualization needs to be augmented with automated visualization techniques.

A few recent systems have attempted to automate some aspects of data analysis
and visualization. Profiler is one such automated tool that allows analysts to
detect anomalies in data \cite{DBLP:conf/AVI/KandelPPHH12}.
% Our work is also similar to VizDeck which is a tool that given a dataset, uses a
% set of pre-determined rules to create diverse visualizations and
% allows the user to pick and choose the visualizations that seem relevant
% \cite{DBLP:conf/sigmod/KeyHPA12}.
% Thus, while powerful, VizDeck requires much more manual input than \SeeDB. 
% In addition, the visualizations generated by VizDeck do not leverage the
% context of the underlying dataset, making the visualizations generated by
% both systems very different in flavor. 
% It would be instructive to augment
% VizDeck visualizations with \SeeDB\ visualizations to study their relative
% utility.
Another related tool 
is VizDeck~\cite{DBLP:conf/sigmod/KeyHPA12}, which, given a dataset,
depicts all possible 2-D visualizations on a dashboard that the user can
manipulate by reordering or pinning visualizations.
Given that VizDeck generates all visualizations, it is only meant for 
small datasets; additionally, the VizDeck does not discuss techniques
to speed-up the generation of these visualizations. 

Statistical analysis and graphing packages such as R, SAS and Matlab could also
be used generate visualizations, but they lack the ability to filter and
recommend ``interesting'' visualizations. 

There has been some recent work on
scalable visualizations in the information visualization community. Immens~\cite{2013-immens} and Profiler (mentioned above) maintain a data cube in memory and use it to support rapid user interactions. While this approach is possible when the dimensionality
and cardinality is small (e.g., simple map visualizations of a single
attribute) it cannot be used with large tables and ad-hoc queries.
Pre-computation and pre-fetching are two other techniques that have
been used for scalability, e.g., \cite{hotmap} uses
precomputed image tiles for geographic visualization,
\cite{doshi2003prefetching} uses extensive pre-fetching and caching. 
A recent paper \cite{2014-viz-latency} discusses how high
latency in visualization systems reduces the rate at which users observe,
analyze and draw conclusions from data, thus making a strong case for
interactive response times.

Finally, finding interesting visualizations in data also involves understanding
user preferences. 
In future work, we plan to learn user preference models towards visualizations
using techniques similar to \cite{CHI:YangLZ14, IUIGotzW09}. 


% \agpneutral{Other recent work has addressed other aspects of visualization
% scalability, including prefetching and caching~\cite{doshi2003prefetching}, data
% reduction~\cite{burtini2013time} leveraging time series data
% mining~\cite{esling2012time}, clustering and
% sorting~\cite{guo2003coordinating,seo2005rank}, and dimension
% reduction~\cite{Yang:2003:VHD:769922.769924}. These techniques are orthogonal to
% our work, which focuses on speeding up the computation of a single visualization
% online.}\\

\stitle{Data Cubes:}
OLAP cubes generalize the idea of group-bys and compute aggregates for all
possible combinations of dimensions in a table.
There has been much research in the area of efficiently computing OLAP cubes 
~\cite{DBLP:jounral/DMKD/GrayCBLR97,  DBLP:conf/VLDB/AgarwalADG96} as well as 
using data mining techniques to aid in exploration of the cube 
~\cite{DBLP:conf/vldb/Sarawagi99, DBLP:conf/vldb/SatheS01, DBLP:conf/vldb/Sarawagi00, 
DBLP:conf/SIGKDD/OrdonezC09}. 
The views considered by \SeeDB, namely, single aggregate group-bys, are a subset of the 
aggregates that make up an OLAP cube. 
The problem addressed by \SeeDB\ is different from traditional cube problems in two important 
ways: 
(1) since \SeeDB\ queries are ad-hoc, all computation must happen online, there is
little room for pre-computation.
(2) since \SeeDB\ must find the most interesting views, \SeeDB\ routinely evaluates hundreds of 
views; cubes with hundreds of attributes are unheard of, they are usually limited to low tens 
of attributes. 
Our DBMS optimizations from Section \ref{} draw from ~\cite{DBLP:jounral/DMKD/GrayCBLR97, 
DBLP:conf/VLDB/AgarwalADG96} to efficiently compute a large number of views. 
However, as seen in the experimental evaluation, these techniques have limited scalability for
large number of views and we must resort to the heuristic techniques of Section \ref{}
for interactive response times.

\cite{DBLP:conf/EDBT/SarawagiAM98, DBLP:conf/vldb/Sarawagi00} among others have explored 
the question of finding ``interesting'' cells in a cube.
Similar to \SeeDB, the ``interesting''-ness of a cell is defined by how surprising
its value is given the other values in the cube.
\cite{DBLP:conf/EDBT/SarawagiAM98} uses techniques based on the table analysis method and
\cite{DBLP:conf/vldb/Sarawagi00} uses techniques based on entropy to find interesting cells.
While these questions are of a similar flavor to the one addressed by our work, \SeeDB\ operates
in a different context.
First, instead of comparing individual cells, \SeeDB\ evaluates sets of aggregates (i.e. distributions
for our view), thus focusing on trends in values rather than individual values.
Second, instead of finding ``intrinsically'' interesting aggregates, \SeeDB\ evaluates aggregate views
with respect to a comparison dataset. 
Because of these fundamental differences, the techniques in prior work such as \cite{DBLP:conf/EDBT/SarawagiAM98, 
DBLP:conf/vldb/Sarawagi00} cannot be applied in our context.
We note however that incorporating an ``intrinsic'' interesting-ness metric into our utility function is an
avenue for future work.

Another work by Sarawagi in this space (\cite{DBLP:conf/vldb/Sarawagi99}) proposes techniques to explain an
increase or decrease in a specific aggregate by drilling down into that aggregate.
Once again, while related, this work addresses a different question; \SeeDB\ seeks to find interesting
differences between two datasets rather than explaining known differences. 

% The work done in \SeeDB\ is similar to previous literature in
% browsing OLAP data cubes. 
% Instead of building complete data cubes,
% one can think of \SeeDB\ views as projections of the cube along various
% dimensions.
%  Data cubes have been very well studied in the literature
% \cite{DBLP:conf/SIGMOD/HarinarayanRU96, DBLP:jounral/DMKD/GrayCBLR97}, and work such as
% ~\cite{DBLP:conf/vldb/Sarawagi99, DBLP:conf/vldb/SatheS01,
% DBLP:conf/vldb/Sarawagi00, DBLP:conf/SIGKDD/OrdonezC09} has explored the
% questions of allowing analysts to find explanations for trends, get suggest for
% cubes to visit, identify generalizations or patterns starting from a single
% cube. 
% This literature is not directly applicable to our problem since the cubes we
% are considering have 10s to 100s of dimensions, making traditional cube
% algorithms infeasible. 


% \stitle{General Purpose Data Analysis Tools:}
% Our work is also related to data mining and the work on building general purpose
% data analysis tools on top of databases. 
% For example, MADLib \cite{DBLP:conf/VLDB/HellersteinRSWF12}
% implements various analytic functions inside the database. 
% MLBase similarly
% \cite{DBLP:conf/CIDR/KraskaTDGFJ2013} provides a platform that allows users to
% run various machine learning algorithms on top of the Spark system
% \cite{DBLP:conf/SCC/ZahariaCFSS10}.



\stitle{Other:}
The techniques we use in our custom implementation of \SeeDB\ draw upon work
from top-k ranking, statistical sampling and the multi-armed bandit strategies. 

The confidence interval-based technique discussed in Section \ref{sec:confidence_interval} 
draws inspiration from the seminal top-k ranking work by Fagin and others in 
\cite{DBLP:conf/pods/FaginLN01, DBLP:conf/vldb/IlyasAE04}. The technique we use is broadly
applicable and has been previously applied to probabilistic top-k ranking~\cite{DBLP:conf/ICDE/ReDS07} 
and rapid sampling for visualizations~\cite{DBLP:journals/corr/KimBPIMR14}. \mpv{References for statistical
sampling}

Similarly, multi-armed bandits (referenced in Section \ref{sec:multi_armed_bandit}) form 
a rich area of research having applications from ad auctions to reinforcement learning. 
Our problem presents a novel application of multi-armed bandit strategy to data analysis\mpv{??} 
and visualization.
The technique we adopt is related to the original UCB algorithm \cite{AuerCF02, LaiR85}
as well as recent work related to the top-$k$ MAB variant \cite{BubeckWV13,
audibert2010best}. \mpv{revisit papers -- how is it related?}

