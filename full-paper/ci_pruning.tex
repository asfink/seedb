\section{Confidence Interval Pruning}
Now, we discuss the assumption that we made early on.
The equation described above apply only in the case
where every time a record is read, an unbiased sample
of the utility is drawn from a distribution which
is assumed to be normal. 
This is not quite the case, since each sample (i.e., each record)
doesn't give us an unbiased sample of the utility;
it instead gives us an unbiased sample for the aggregate
of a single group within the view, thereby indirectly affecting
the utility.
For instance, if we read a record corresponding to 
``Airline = UA'', this affects only the ``UA'' group
for a view where we are estimating flight delay grouped by airline.
We now propose two modifications to the equation above to address this issue.
The first modification we make has to do with how we define utility.
Recall from Section \ref{sec:problem_definition} 
that the utility of a view is
defined as the distance between two distributions: 
the distribution of aggregate values for the
target view and the distribution of aggregate values for the comparison view.
These distributions are in turn tied to the 
number of distinct groups present in
each dimension attribute.
For our purposes, it means that if a dimension attribute has $g$ distinct
groups, then a sample with $x$ rows gives us approximately $\frac{x}{g}$ 
values for each group (assuming uniform distribution).
Said another way, a sample with $x$ rows for the purpose of computing 
utility is essentially only giving us a sample of $\frac{x}{g}$ rows.
So the first modification we 
make to Equation \ref{eq:confidence_interval} is to
replace $m$ by $\frac{m}{G_{max}}$ where $G_{max}$ is the maximum number of
distinct groups present in any dimension attribute.
Second, our mean and variance in the utility needs to be calculated 
over a sample of utilities. 
In our case, since we are not getting samples of utilities, 
and instead getting samples from individual groups that contribute
to utility, we do the following. 
Within a phase, we take the estimate of the utility after every
record is read, and then we compute the mean and variance on these estimates.
Since utility estimates only improve as we get closer to reading 
the entire dataset, we drop the estimates of mean and variance at the end
of every phase; additionally, the number of rows read, $m$, is set to $0$
at the end of each phase as well. This allows us to get estimates
of means and variance, using which, we can apply the standard 
confidence interval bounds.
