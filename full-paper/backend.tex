%!TEX root=document.tex

\section{View Pruning}
\label{sec:pruning}

The previous sections described the \VizRecDB\ execution engine, the core part
of our system.
In this section, we briefly discuss an important component of \VizRecDB\ that is invoked
even before the execution engine runs, namely the view generator.
Given a user query $Q$, the purpose of the view generator module is to take the
input query, obtain metadata about the underlying tables and use correlations
between columns to prune views whose evaluation is
unnecessary. 
This step is important because fewer views means that we can
obtain even better performance from our execution engine.
We distinguish between the pruning done in the execution engine and the pruning
done in the view generator: the execution engine prunes away low-utility views
while the view generator prunes away {\it redundant} views. 
A set of views $\{V\}$ is said to be redundant if all views in $\{V\}$ have
similar distributions and are therefore expected to have similar utility.
An extreme example of redundant views is the following: a table stores the sales
of a product as measured in US Dollars and measured in Euros. 
Views with either of these column as the measure attribute will be identical and
have the same utility.
Similarly, if a table has two dimension attributes, one corresponding to the
airport name and another corresponding to the airport code, views with either
of these attributes as dimensions are guaranteed to produce identical views.
In both of the above cases, it suffices to compute and show only a single view
that is representative of multiple views (and list redundant
views that have been omitted).

The view generator largely works offline. At run time, it merely accesses
previously computed view stubs, removes any views that contain
attributes from the selection predicte of the incoming query and sends remaining
view stubs to the execution engine.
The offline pruning works as follows: the view generator uses metadata on each
attribute to first determine the entire space of views. 
It removes all views containing attributes with 0 or low variance.
Then it computes full-table aggregates corresponding to all views, i.e., for
each dimension $a$ and measure $m$, it computes the corresponding aggregate over
group-by query and computes the normalized distribution.
Next it computes the correlation between distributions corresponding to each
pair of views (note that the dimensions must have same cardinality for this
comparison to be valid).
Correlation scores are thresholded and used to cluster views into groups.
Finally, we pick a single view from every view cluster.

Details of our pruning algorithm are presented in Appendix
\ref{sec:view_pruning} along with examples of view clusters.
View pruning allows us to significantly reduce the number of views in real
datasets. 
For instance, for the BANK dataset described in Section \ref{sec:experiments},
we reduce the number of possible views from AAA to BBB and in the DIAB dataset
from CCC to DDD. 



% In this section, we discuss an important component of \VizRecDB\ that is invoked
% even before the Execution Engine runs, namely the View Generator.
% Given a user query $Q$, the purpose of the view generator module is to take the
% input query, obtain metadata about the underlying tables and use correlations
% between different columns in the table to prune views whose evaluation is
% unnecessary. 
% We distinguish between the pruning done in the execution engine and the pruning
% done in the view generator: the execution engine prunes away low-utility views
% while the view generator prunes away {\it redundant} views.
% So what are redundant views? Redundant views are views that have similar
% distributions and are therefore expected to have similar utility.
% An extreme example is that of sales of a product as measured in US
% Dollars and measured in Euros. Views with these measure attributes will be
% identical and have the same utility.
% Similarly, two dimension attributes, one corresponding to the airport name
% and another corresponding to the airport code are guaranteed to produce identical
% views irrespective of the measure attribute.
% In both of the above cases, it suffices to compute and show only a single view
% that is representative of multiple views (the frontend does list redundant
% views that have been omitted).

% The View Generator works in two stages: it performs view pruning offline and
% identifies the set of viable views; then, when a new user query comes in, it
% reads the set of viable views, performs pruning based on the input query and
% passes view stubs on to the Execution Engine. 
% The offline pruning does not depend on the input query and can therefore be
% perfomed only once.
% The offline stage works as follows. 

% First, for each table, the View Generator obtains various types of
% metadata including the data types of attributes, their classification into
% measure and dimension attributes, number of distinct values for dimension
% attributes, and the distributions for each measure attribute (mean, std
% deviation).
% The first piece of metadata is essential to determine the full space of
% views. 
% The number of distinct values for dimension attributes and the variance for
% measure attributes is used to perform basic pruning of views (e.g. views
% containing zero variance attributes will have low utility).

% \mpv{If two dimension attributes $a_i$ and $a_j$ have
% a high degree of correlation (e.g. full name of airport and abbreviated name of
% airport), the views generated by grouping the table on $a_i$ and $a_j$ will be
% very similar (and have almost equal utility). We can therefore generate and
% evaluate a single view representing both $a_i$ and $a_j$. \VizRecDB\ clusters
% attributes based on correlation and evaluates a representative view per
% cluster.}



  
% We next describe a scheme that allows us to associate upper and lower bounds for
% views by evaluating them on a small sample of the dataset.
% We describe the use of the scheme on a simple view where AVG(Y) for a given
% attribute Y is being computed for each group in attribute X.
% We can then depict this view using a bar chart or a histogram.
% 
% For this derivation, we assume that the AVG(Y) for any X = $x_i$, is normally
% distributed around a certain mean $p$.
% Given a number of samples for Y for X = $x_i$, we can employ the following
% theorem \cite{stats_book} to bound $p$ within a confidence interval with
% probability $1 - \delta$:
% \begin{theorem}~\label{thm:confint}
% If $\hat{p}$ and $s$ are the mean and standard deviation 
% of a random sample of size $n$ from a normal distribution with unknown 
% variance, a $1 - \delta$ probability confidence interval
% on $p$ is given by:
% $$\hat{p} - \frac{t_{\delta/2, n-1} s}{\sqrt{n}} \leq p \leq \hat{p} + \frac{t_{\delta/2, n-1} s}{\sqrt{n}}$$
% where $t_{\delta/2, n-1}$ is the upper 100$\alpha/2$ percentage point
% of the $t$-distribution with $n-1$ degrees of freedom.
% \end{theorem}
% 
% Now, we demonstrate how we can use this theorem to establish an upper 
% and lower bound for the utility of a view, with probability $1 - \delta$.
% 
% Let the distance vector corresponding to the target view be:
% $\bar{a} = [a_1, a_2, \ldots, a_k]$ while the distance vector corresponding to
% the comparison view is:
% $\bar{b} = [b_1, b_2, \ldots, b_k]$.
% Notice that on very large datasets, it may be beneficial to precompute the
% distance vectors corresponding to the comparison views, so we assume that the
% vector $\bar{b}$ is computed exactly and known in advance.
% We let $a = \sum_i a_i$, and $ b = \sum_i b_i$.
% 
% Our goal is to use the sample to bound the values of the $a_i$ around $\ha_i$
% such that we can establish upper and lower bounds for the utilities.
% By applying Theorem~\ref{thm:confint}, we
% can get values $c_i$ for which $a_i \in [\ha_i - c_i, \ha_i + c_i]$
% with probability greater than $1 - \delta/k$.
% (By union bound, we will be able to ensure that all $a_i$'s
% are in their intervals with probability $1 - \delta$.)
% 
% Now, given these values $c_i$, we can establish an upper bound for the
% EMD (and also similarly for other distance metrics) in the following manner:
% We let $q_1(\bar{a}) = \sum_i \ha_i - c_i$, and $q_2(\bar{a}) = \sum_i \ha_i + c_i$.
% 
% 
% \begin{align*}
% EMD(\bar{a}, \bar{b}) & = \sum_i |a_i / a - b_i / b|\\
%           & = \sum_i |a_i / a - b_i / b|\\
%           & = 1/ab \sum_i \max (a_ib  - b_ia, b_ia - a_ib)\\
% \end{align*}
% Thus, we have:
% \begin{align}
% \frac{1}{b q_1(\bar{a})} \sum_i \max (a_ib  - b_ia, b_ia - a_ib) \leq & EMD(\bar{a}, \bar{b}) \leq \frac{1}{b q_2(\bar{a})} \sum_i \max (a_ib  - b_ia, b_ia - a_ib)\label{eq:emd}
% \end{align}
% 
% Note that: 
% \begin{align*}
% (\ha_i - c_i)b  - b_i (\sum_i (\ha_i + c_i)) & \leq a_ib  - b_ia  \leq (\ha_i + c_i)b  - b_i (\sum_i (\ha_i - c_i)), \textrm{\ and} \\
% b_i (\sum_i (\ha_i - c_i)) - (\ha_i + c_i) b & \leq b_i a  - a_i b  \leq  b_i (\sum_i (\ha_i + c_i)) - (\ha_i - c_i) b
% \end{align*}
% By plugging these quantities back into Eq~\ref{eq:emd},
% we have upper and lower bounds on the EMD metric.
% Similar mechanisms may be used to derive upper and lower bounds for other metrics.
% 
% Now that we have upper and lower bounds for the utility of each target view
% by evaluating the query on a sample,
% we can easily use it to prune away a number of views that are definitely not likely to be part 
% of the top-K,
% and instead focus on views that may be part of the top-K.

%   \subsection {Partitioning Tables}
%   The increase in the total execution time when a large number of queries are
%   executed in parallel suggests that there is a ``sweet spot'' with respect to
%   the maximum number of queries that can be run in parallel on a given table.
%   Therefore, we uniformly partition large tables into smaller ones and run
%   subsets of queries against each of the partitions. Note that the views
%   returned are nor approximate because we are now executing views against
%   subsets of the data. As a result, bounds developed in sampling now apply. We

 


%If a dimension attribute $\mathcal{d}$ is highly correlated with measure
  %attribute $\mathcal{m}$, then?

% \mpv{also from full paper draft}
% It is possible to collect the above statistics at the dataset level too, as
% opposed to the entire table level. The advantage of table level statistics is
% that they have to be computed only once per table; however, dataset-level
% statistics are more accurate since they only consider the specific parts of the
% table. XXX: we use dataset-level statistics with table statistics do not result
% in aggressive pruning. 



