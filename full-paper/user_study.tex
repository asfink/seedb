%!TEX root=document.tex

\section{User Study}

We demonstrate the utility of \SeeDB\ through a user study on MTurk as well as
through in person interviews with data analysis experts. 
Our goals with the user study were as follows: (1) to validate the \SeeDB\
technique of identifying interesting views via deviation; (2) to evaluate the
relative merit of different distance metrics; and (3) to evaluate the \SeeDB\
tool as a whole.

To study questions (1) and (2), we created an MTurk study where 30 turkers were
provided with a questionnaire containing 20 visualizations. These visualizations
had differing utilities and varying number of distinct values on the X axis
(see Figure \ref{}) for examples of visualizations. For each visualization, we
asked subjects to rate (on a scale of 1 to 5) if the visualization showed
something interesting or insightful about the underlying data. We removed the X
and Y axis labels to avoid confusing the subjects. The results are shown in
Table \ref{}. We see that XXX. 

We also used the data from this study to evaluate the
merit of different kinds of utility metrics. Specifically, we ranked the set of
visualizations shown to the subjects using different metrics (see Section
\ref{}).
We then compared the ranking of visualizations generated by the metrics to the
rating provided by the users. 
To convert the rankings to ratings, we divided the ranked list of visualizations
into 5 bins. The top ranked bin corresponded to a rating of 5 while the lowest
ranked bin corresponded to a rating of 1.
We see that XXX.

To study the utility of \SeeDB\ as a whole, we conducted in-person interviews
with XXX experts in data analysis. We adopted the following protocol: 

% \stitle {Scenario 1: Demonstrating Utility.} Attendees are provided with three
% diverse, real-world datasets to explore using \SeeDB. For each dataset,
% attendees can issue ad-hoc or pre-formulated queries to \SeeDB. \SeeDB\ will
% then intelligently explore the view space and optimize query execution to return the
% most interesting visualizations with low latency. Attendees can examine the
% returned queries visually, via the associated view metadata, and via
% drill-downs. To aid the evaluation of visualizations, the demo system will 
% be configured to also show the user ``bad'' views (views with low utility) that were not selected
% by \SeeDB.
% Similarly, we provide pre-selected queries (and
% previously known information about their results) to allow attendees to
% confirm that \SeeDB\ does indeed reproduce known information about these
% queries. Attendees will also be able to experiment with a
% variety of distance metrics for computing utility and observe the effects on the
% resulting views.

% \stitle{Scenario 2: Demonstrating Performance and Optimizations.} This scenario
% will use an enhanced user interface and synthetic datasets mentioned above.
% Attendees will be able to easily experiment with a range of synthetic datasets and input
% queries by adjusting various ``knobs'' such as data size, number of attributes, and
% data distribution. In addition, attendees will also be able to select the
% optimizations that \SeeDB\ applies and observe the effect on response times and
% accuracy.

% Thus, through our demonstration of \SeeDB\, we seek to illustrate that (a) it is
% possible to automate labor-intensive parts of data analysis, (b) aggregate
% and grouping-based views are a powerful means to identify interesting trends
% in data, and (c) the right set of optimizations can enable real-time data
% analysis of large datasets.