%!TEX root=document.tex

\section{Evaluation}
\label{sec:user_study}

we tested two things: the deviation metric
and the system

\subsection{Validating Deviation-based Distance Metric}
\label{sec:validating_metric}

- we got ground truth from people; exhaustively label data
- see how well we can reproduce it

User preferences

New utility metrics

\subsection{\SeeDB vs. Manual Chart Construction Tool}
\label{sec:seedb_vs_manual}

To assess the efficacy of \SeeDB in enabling faster visual analysis,
we conducted a user study where participants performed visual analysis
using both \SeeDB and a manual chart construction tool.
We hypothesized that: (i) When using \SeeDB, participants would find 
interesting visualizations {\em faster} than when using manual chart
construction, (ii) Participants would find more interesting visualizations
when using \SeeDB vs. when using manual chart construction, (iii) 
Participants would prefer using a tool with recommendations vs. a manual
construction tool.

\stitle{Participants}. We recruited 16 participants (5 female, 11
 male) all graduate students with prior data analysis experience.
 All subjects had previous experience with visualization tools (e.g.
 R, ggplot, Excel).
 None of the participants had previously worked with any of the study datasets.
 The study lasted \~an hour and participants were compensated with a \$15
 gift card.

 \stitle{Datasets}. We used two datasets during our study:
 (1) {\em Housing} - Dataset of housing prices in
 the Boston area comprising crime rate, racial diversity, housing prices and
 income indicators; 
 (2) {\em Movies} - Dataset about movies containing information about gross
 sales, genres, and ratings;
 These two datasets were chosen because they were easy to understand and 
 their topics that would be familiar to participants. 
 In addition, the datasets were comparable in terms of number of rows and 
 potential visualizations.

\stitle{Study Protocol}.
Our user study used a 2 (visualization tool) X 2 (dataset) 
within-subjects design.
The tools used were \SeeDB and {\em MANUAL}-{\em only} version of \SeeDB.
MANUAL was the same as \SeeDB except that the recommendations bar was 
removed (Figure \ref{}).
Using the same underlying tool in both conditions allowed us to control for
tool-related factors such as functionality and user interface.
We used a within-subjects design to compensate for difference in data analysis
expertise within subjects, and used counterbalancing to remove any effects 
related to order and the test dataset.

Our study began with a short tutorial on the two tools used in the study.
The tutorial used a dataset different from the study datasets and acquainted
the user with the functionality of both tools.
Following the tutorial, participants were asked to perform two visual analysis 
tasks, one with each tool (condition).
In each condition, we introduced participants to the test dataset
and described the analytical prompt using written instructions.
Each analytical task asked participants to use the specific tool to find 
visualizations supporting or disproving a specific hypothesis.
Participants were asked to use the bookmark button (XXX in Figure \ref{}) to 
flag any visualizations they deemed relevant for the particular task.
Participants were also encouraged to think aloud and explain their thought process
while performing the task.
Since the analytical tasks were open-ended, we capped each task at 8 minutes.
After each task, participants filled out a short survey about their experience
using the particular tool.
Most questions were answered on a 5-point Likert scale.
All studies were conducted in a lab setting using Google Chrome on a 15-inch 
Macbook Pro.

\stitle{Methods and Metrics}.
Over the couse of each study session, we collected data by three means: interaction logs 
from each tool, study staff notes, and responses to surveys.
Since users were asked to bookmark visualizations they found to be relevant to the task,
bookmarking behavior, recovered from tool interaction logs, provides the richest source
of information about the analytical process.
Specifically, we study a number of metrics including: (i) number of bookmarks ($num\_bookmarks$), 
(ii) total number of visualizations viewed ($total\_viz$), 
(iii) bookmarking rate ($bookmark\_rate$) defined as $num\_bookmarks$/$total\_viz$, and 
(iv) the time to bookmark ($bookmark\_time$).
\SeeDB and MANUAL support construction of two kinds of charts: aggregate visualizations and 
scatterplots (to replicate real visual analysis).
Since \SeeDB can only recommend aggregate visualizations, we also examine the corresponding
metrics for aggregate visualization only.
We evaluate statistical signifance of our results using paired t-tests and ANOVAs.

% We hypothesize that if a visualization is high-quality, it is more likely to be bookmarked.
% This implies that high-quality recommendations can lead to a larger number of bookmarks and
% an increase in the bookmarking rate.

\stitle{Results}.
Over the course of our study, participants built over 250 visualizations and bookmarked 70
visualizations.
The overall bookmark rate was 0.26.
We next describe our key findings and observations.

\stitle{1. \SeeDB enables fast visual analysis}.
Table \ref{tab:bookmarks} shows an overview of the bookmarking behavior divided by tool.
We find that, in general, participants bookmarked slightly more visualizations with \SeeDB than 
with MANUAL.
On the other hand, we find that participants interacted with fewer visualizations in \SeeDB than
in MANUAL.
As a consequence of these two opposing forces, we find that participants using \SeeDB view fewer
visualizations but bookmark more, i.e., the visualizations they interact with are high quality.
In fact, the $bookmark\_rate$ for \SeeDB is 1.5X higher than that for MANUAL.
We also find that, on average, the $bookmark\_time$ for \SeeDB (92.91 $\pm$ 49.26) is twelve seconds 
shorter than that for MANUAL (105.02 $\pm$ 58.24). 
While these differences are not statistically significant, they point towards a trend: {\it \SeeDB
enables participants to arrive at interesting visualizations faster than MANUAL}.

\begin{table}[htb]
  \centering \scriptsize
  \begin{tabular}{|c|c|c|c|c|} \hline
   & num\_bookmarks & total\_viz & bookmark\_rate \\ \hline
  MANUAL & 3.3 $\pm$ 1.42 & 14.1 $\pm$ 5.4 & 0.24 $\pm$ 0.09 \\ \hline
  \SeeDB & 3.5 $\pm$ 1.35 & 12.1 $\pm$ 4.7 & 0.36 $\pm$ 0.22 \\ \hline
  \end{tabular}
  \vspace{-10pt}
  \caption{All Visualizations: Bookmarking behavior Overview}
  \label{tab:bookmarks} 
  \vspace{-10pt}
\end{table}

Recall that \SeeDB (currently) only supports recommendations for aggregate visualizations.
Therefore views of all visualizations do not necessarily capture the behavior with respect to
aggregate visualizations.
When we examine the same metrics with respect to aggregate visualizations, we find a clear
signal.
The rate of bookmarking aggregate ($agg\_bookmark\_rate$) visualizations in \SeeDB (0.42) is 3X that
in MANUAL (0.14).
The 3X increase is larger than that seen in all visualizations (1.5X) and is statistically significant 
within subjects as well as across subjects ({\em Paired t-test, t = -2.5599, df = 8, p-value = 0.03365}).
We note that there is significant difference also between the number of aggregate bookmarks in the
two conditions.
However, the absolute number of aggregate visualizations and therefore aggregate bookmarks is closely
tied to the relative proportion of scatterplots and aggregate visualizations.
As a result, the absolute numbers can be biased and we ignore them in spite of their statistical
significance.
Finally, we note that a 2-way ANOVA also concurs that the tool has a significant impact on aggregate 
bookmark rate ({\em df = 1, sum sq = 0.3681, mean sq = 0.3681, F value = 10.034, p = 0.00685}). 
We find that choice of dataset does not affect bookmark rate, and there are no interaction or order effects.

User survey data also supports the claim that \SeeDB enables fast analysis.
87\% of participants indicated that \SeeDB recommendations sped up their visual analysis.

\stitle{2. \SeeDB found interesting and unexpected visualizations}.
One of the goals of \SeeDB is to find interesting visualizations that the user may not generate, i.e.,
we want to surface unexpected visualizations.
We find evidence for this in the study.
Figure \ref{} shows an example of visualizations that were not generated by participants using MANUAL
but was in fact recommended by \SeeDB and bookmarked as being interesting.
78\% of participants also indicated that \SeeDB recommended visualizations they might not explored, e.g.
``\ldots interesting aspects of data to compare. I don't think I would have checked those by myself.''.


\stitle{3. \SeeDB provides starting point for analyses}. 
To our knowledge, \SeeDB is the first tool to provide recommendations for supporting visual
analysis.
As a result, we were interested in how recommendations could fit into the analytical workflow.
While a participant's exact workflow was unique, we repeatedly found the pattern {\em recommendation 
$\rightarrow$ manual\_modify $rightarrow$ manual\_modify $rightarrow$ \ldots bookmark}.
Specifically, participants would often start from one of the recommendations and explore other
visualizations that were variations of it (e.g. different aggregation or measure attribute) 
until they found an interesting visualization.
\mpv{can I put any data here?}
Thus, even if recommendations weren't bookmarked directly, their variations were often bookmarked.
This pattern was highlighted in user comments as well. 
8 users highlighted the use of \SeeDB in {\em seeding} their analysis: e.g.,
``\ldots would be incredibly useful in the initial analysis of the data'', 
``Exploratory analysis \ldots'',
``\ldots great tool for proposing a set of initial queries for a dataset'',
``\ldots quickly deciding what correlations are relevant and gives a quick peek''.
On the other hand, we found participants are much less satisfied with recommendations when they 
are unable to interact with or modify them.
These observations reinforce the design choice of \SeeDB as a complement to a traditional
visualization system; the mixed-initiative nature of the system is essential for the system to be
userful in analysis.

\stitle{4. All participants preferred \SeeDB to MANUAL}. 
The most important result of our survey was that 100\% of all users preferred \SeeDB to MANUAL for
visual analysis, i.e., all users preferred to have recommendations while carrying out visual analysis.
As discussed previously, 78\% of participants found the recommendations helpful and thought that they
showed interesting trends.
An interesting observation from two participants was that they did not want to rely too heavily
on the recommendations; one participant noted {\em ``The only potential downside may be that it made 
me lazy so I didn't bother thinking as much about what I really could study or be interested in''}.
This observation suggests that there is a high bar on quality for recommendations, and an interesting
line of future work could be to explore means to build confidence in recommendations.

% 86\% of participants indicated that the recommendations sped up their analysis.


% When asked to rate the recommendations provided by \SeeDB, 78\% participants indicated that the
% recommendations were either ``Helpful'' or ``Very Helpful''. 
% We also found that 90\% of participants found the comparative visualizations shown by \SeeDB 
% helpful in their analysis.
% 66\% of participants indicated that the recommendations needed improvement, in particular,
% participants were interested in seeing different types of charts (e.g. geographical, time series)
% and obtaining measures of statistical significance.

% \stitle{Qualitative Feedback}. In their qualitative feedback, participants highlighted the importance of 
% a tool like \SeeDB at the initial stages of analysis. 
% One partitipant said, {\em ``It's a great tool for proposing a set of initial queries for a dataset I have never seen. 
% And from these visualizationns, I can figure out which related patterns to dig into more.''}
% Others thought that the strength of the tool was in quickly finding relevant trends, {\em ``It's a good tool that helps 
% in quickly deciding what correlations are relevant and gives a quick peek''}. 
% Overall, participants indicated that \SeeDB was particularly suited for exploratory analysis of new datasets, 
% {\em ``I thought SeeDB was very helpful in helping me get more familiar with a new dataset quickly.''}.



