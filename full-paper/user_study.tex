%!TEX root=document.tex

\section{User Study}
\label{sec:user_study}

Our \SeeDB prototype currently provides 
visualization recommendations based on the 
deviation between the query and the background data.
Our goal in this section is to study how
far we can go even with these visualization
recommendations, i.e., how useful are the 
visualizations generated by \SeeDB, on an absolute scale
and compared to baselines. 
We achieve this via three user studies.
The first study tests our hypothesis that large 
{\it deviations in distribution} 
lead to high utility visualizations;
the second compares \SeeDB's data-driven recommendations 
to baseline recommendations; and the third
determines how data-driven 
recommendations can contribute to 
holistic visualization recommendations. 

\reviewer {
	More convincing evaluation using actual data scientists is needed
}

\reviewer {
	D3.4 It would be interesting to see how SeeDB compares to other techniques
proposed to support database exploration (see the second remark about related work below)
}

\reviewer {
	W3: Discussion on actual (new) findings and insights generated using
SeeDB would have been great. Do the authors have examples for a dataset
where the system was even able to identify unexpected visualizations, the
analyst would not have thought of? This would further strengthen the paper
and would even highlight the usage of the system for expert users which
know their dataset.
}


\stitle{Study I: Deviation Between Distributions.}
\reviewer{
	D5 The users of mTurk might not be a good representative sample of users of
SeeDB ( as one can see, they have rated the random views very high almost
as high as your approach).
D6 There are no details given on how the authors checked if an mTurk user is
a quality worker. This is standard when using paid workers. If no quality
checks were done (i.e., the turkers could be giving random answers just to
get paid), it is hard to understand the value of these user studies.
}

\reviewer{
	D3.2 I did not fully understand user test 1: how do users know about the
deviations in the visualization they are presented? Can we be sure that more
expert analysts would have the same inclination towards deviation?
}

{\it \underline{Aim}}: Test whether the {\it deviations between distributions} metric leads to useful visualizations.
\noindent {\it \underline{Participants}}: 12 participants on Mechanical Turk (MTurk).
\noindent {\it \underline{Methodology}}: 
We conducted a within-subjects experiment comparing 16 pairs of visualizations generated 
from the DIAB dataset (Section \ref{sec:experiments}) 
where each visualizations in a pair showed the same dimension and measure attributes (thus minimizing effect of context) but had varying degrees of deviation. 
(The procedure for constructing comparable pairs of visualizations is described in the tech report~\cite{seedb-tr}.)
Participants were provided a brief background on the DIAB dataset prior to the study.
They were then presented with 16 pairs of visualizations. 
For each pair, participants were asked to choose the visualization that seemed more {\it interesting, insightful or valuable} to them.
We recorded the number of times that the visualization with higher deviation was selected as being more interesting, and vice versa (denoted $N_h$ and $N_l$ respectively).
\techreport{To generate comparable pairs of visualizations, 
we adopted the following procedure.
We randomly chose eight unique <dimension, measure> pairs from the DIAB dataset.
For a selection query on the race attribute, we constructed for each <dimension, measure> pair, three hypothetical visualizations corresponding to varying degrees of deviation between the target and comparison distributions, specifically, low, medium, and high deviation.
We then randomly selected two pairs of visualizations from the three generated above (e.g. high-low and low-medium).
Since both visualizations in each pair showed the same dimension and measure attributes, we minimized the effect of context on participants' preferences.}\\
\noindent {\it \underline{Results}}: The 12 MTurk workers took on average 
1 min 46 sec to compare the 16 pairs of visualizations.
The average $N_h$ across all participants was 10.08 and $N_l$ was 4.
Under the null hypothesis (that deviation is irrelevant to utility of visualization),
there would be no difference between $N_{h}$ and $N_{l}$.
However, we find that the {\em distribution of $N_{h}$ and $N_{l}$ is statistically different with
p-value 0.01}.
This finding verifies our hypothesis that, on average, participants 
find visualizations with deviation more interesting and useful.


% Next, for each view, we presented the subject with two visualizations randomly chosen from the three described above.
% The subject was asked to choose the visualization that seemed more
% valuable or interesting.

% Since the axes for each pair of visualizations were identical, we minimized the effect {\it context} would have on user preferences.
% We ran this experiment with 12 subjects.
% Our alternate hypothesis was that visualizations showing large deviation were more interesting than those showing small deviation.
% Table \ref{} shows the results of this experiment.
% $N_{high}$ denotes the number of instances where the subject responded that the visualization with higher deviation was more interesting than the one with low deviation. 
% $N_{low}$ denotes the opposite.
% 

\stitle{Study II: Comparison of Recommendations.}
{\it \underline{Aim}}: Determine if \SeeDB's visualization 
recommendations 
are more useful than other visualization schemes.
\noindent {\it \underline{Participants}}: 12 participants on MTurk.
\noindent {\it \underline{Methodology}}: We performed a within-subjects study comparing the quality of \SeeDB visualizations (via ratings) to the quality of baseline recommendations.
For this experiment, we ran \SeeDB on the DIAB dataset and picked the top-$5$ visualizations returned by \SeeDB.
For baseline recommendations, we randomly selected five visualizations from
the full set of visualizations considered by \SeeDB.
Participants were provided a brief background on the DIAB dataset prior to the study.
Participants were then presented two sets of visualizations: the recommendations from \SeeDB and the baseline recommendations. 
The order of presentation was randomized to avoid order effects.
For each set of visualizations, participants were asked to rate each visualization on a scale of 1 (lowest) - 5 (highest) based on how {\it interesting, insightful or valuable} they found the visualization.
Participants were also asked to provide an overall rating for each set of visualizations.\\
\noindent {\it \underline{Results}}: 
% The average rating of visualizations generated by \SeeDB was 3.475 and that for RANDOM was 3.175 (p-value 0.1).
{\em The average rating for the entire set of \SeeDB visualizations was 3.75 while that for RANDOM was 3.125, and this difference was found to be statistically significant (p-value 0.04)}. 
Thus, we find that overall, participants prefer the set of recommendations provided by \SeeDB over baseline recommendations. 
As we incorporate context and user preferences
into the mix, we expect \SeeDB's visualizations
to become even more useful.

\reviewer {
	D7 In study II an ordinal scale is used for mean computation. This is not
desirable (see a brief argument here → Jamieson, Susan. "Likert scales: how
to (ab)use them." Medical education 38.12 (2004): 12171218.
Apologies for the obscure ref, didn't have time to find a common stats text that discusses
this. I'm in no way suggesting you should have read the medical education
lit, but the fact that ordinal numbers shouldn't be averaged is basic stats
knowledge and common knowledge in the visual analytics community. We are
just DB people but we do need to use stats prudently )
}

\reviewer {
	D8 The necessary information for verifying significance is omitted for example
the standard deviation, and the name of the statistical test that shows
significance.
}


\stitle{Study III: Observational Study Results:}
{\it \underline{Aim}}: Determine how data-driven 
recommendations can contribute to holistic visualization recommendation.
\noindent {\it \underline{Participants}}: 5 data analysis experts (MIT grad students with 3+ years data analysis experience).
\noindent {\it \underline{Methodology}}: We performed an observational study evaluating how \SeeDB could benefit data analysis experts in quickly identifying interesting and useful visualizations.
In this experiment, we began with a pre-test where we introduced the participants to the study dataset and asked them to build interesting visualizations using their software of choice (Excel, R or Matlab).
Participants were encouraged to talk aloud about visualizations they were building and why.
We measured the time taken by each participant to generate what they deemed an interesting visualization.
To control for time, we capped the pre-test at 10 minutes.
Next, the participants were given a tour of \SeeDB and instructed in its use.
Participants were then asked to use \SeeDB to find interesting visualizations of their dataset.
Each participant was asked to evaluate the visualizations generated by \SeeDB and talk aloud about its advantages and shortcomings.\\
\noindent {\it \underline{Results}}:
The major observations from this study were:
 (a) Building valuable visualizations for data analysis is challenging and time-consuming. In the pre-test, participants spent 5+ minutes generating their first {\it interesting} visualization.
(b) In contrast, \SeeDB provided recommendations instantly and produced several of the visualizations participants had attempted to create in the pre-study.  
(e.g. for the DIAB dataset, \SeeDB generated an age vs. number of procedures chart that a participant wanted to create in the pre-test).
% \item Participants rated \SeeDB visualizations highly and agreed that they provided valuable insights. \mpv{cite the ratings they gave seedb}
(c) We found that {\it interactivity} was as important as recommendation quality. Participants wanted to interactively perform perform drill downs and rollups to verify and explore trends.
 (d) Along with data distributions, {\it context and user preferences}  were  important factors in determining visualization utility. A deviation that was interesting for a particular pair of attributes (e.g. age of patient vs. number of procedures) was not interesting for another (e.g. severity of disease vs. number of procedures). 
% due to prior knowledge of the expert.

% The three set of experiments described above provide an evaluation of the quality of \SeeDB results in a real system.
% We found that deviation-based distance metrics are in fact a good metric to identify utility of visualizations.
% We did however also find that in a few, very specific cases, very similar distributions are also valuable (e.g. when the target and comparison views compare disparate datasets).
% Our randomized controlled experiment shows that \SeeDB performs significantly better than the baseline in producing high utility
% recommendations \mpv{data}.

\reviewer {
	D9 In study III it would be good if instead of saying “several of the
visualizations…”, the authors actually give the number of views that have
been generated by both SeeDB and the participants in the prestudy.
}

\reviewer {
	D10 The claim that SeeDB speeds up the process significantly in the
conclusion section of study III is not verifiable since no test of significance or
data is given.
}

\reviewer {
	D3.3 User test 3 is somewhat disappointing: what about the users' feedback on
latency? what about the quality of user experience, their overall satisfaction?
What about changing k? Why not having some analysts using SeeDB while
others not, and then compare their feedbacks? Indeed, it seems to me that
when using SeeDB, the users get more familiar with the dataset, compared to
when they first built their visualizations with excel, and this may introduce a
bias in the test.
}

\reviewer {
	W1: I would have expected more details on the Observational Study Results. To judge the actual benefits also with respect to reveal new insights,
it would have been interesting to see more details and results of actual
suggestions by the system. Did the system only suggest obvious
combinations? How many of them didn't make sense from a semantically
point of view? More discussion about such possible limitations might be
helpful.
}

\stitle{Conclusions.} Our study indicates that automated visualization recommendations can speed up the analysis process significantly, and can be a useful starting point for individuals unfamiliar with the data.
Data-driven recommendations are particularly useful in the absence of semantic information and user preferences.
We also see some directions for improvement: \SeeDB's recommendations can be made even more useful by incorporating context and supporting interactive data exploration.
% We find that holistic recommendations must take the other recommendation dimensions (e.g. context) into account and the visualization system must make it easy for users to interact with the recommended visualizations.
