%!TEX root=document.tex

\section{User Study}
\label{sec:user_study}

Our current \SeeDB prototype currently provides 
simple visualization recommendations based on 
deviation between the query and the background data.
(That said, note that nothing prevents us from using a more general
distance metric---see Section~\ref{sec:general-metric}.)
Our goal in this section is to study how
far we can go even with these simple visualization
recommendations, i.e., how useful are the 
visualizations the \SeeDB generates, on an absolute scale
and compared to baselines. 
We achieve this via a user study. 
% In this section, we describe our user study evaluating 
% the quality of data-driven visualization 
% recommendations produced by \SeeDB.

Our goals in this study were three-fold: 
(I) to test our hypothesis that even just using 
{\it deviation in distribution} 
leads to high utility visualizations;
(II) to compare data-driven recommendations generated 
by \SeeDB to baseline recommendations; and 
(III) to determine how data-driven 
recommendations can contribute to 
holistic visualization recommendations. 
For Goals I and II, we ran two within-subject\cite{} experiments on Mechanical Turk\cite{} evaluating real and synthetically generated visualizations.
For Goal III, we ran an observational study 
with data analysis experts using \SeeDB.
We now describe each of our experiments and results.


% Test absolute utility -- we tested our hypothesis that {\it deviation in distribution} creates high utility views.
% Next, we ran a randomized controlled experiment 
% comparing the quality of recommendations of \SeeDB to baseline
% recommendations.
% Finally, we conducted an observational study with data analysis experts 
% to understand how data-driven recommendations can contribute to
% making high-quality visualization recommendations.

\stitle{Study I:}
{\it \underline{Aim}}: Test whether {\it deviations between distributions} lead to useful visualizations.
\noindent {\it \underline{Participants}}: 20 participants on MTurk.
\noindent {\it \underline{Methodology}}: 
We conducted a within-subjects experiment 
comparing 16 pairs of visualizations generated 
from the DIAB dataset (Section \ref{sec:experiments}) 
and having varying degrees of deviation. 
(The procedure for constructing comparable pairs of visualizations is described below.)
Participants were provided a brief background on the DIAB dataset prior to the study.
They were then presented with 16 pairs of visualizations. 
For each pair, participants were asked to choose the visualization that seemed more {\it interesting, insightful or valuable} to them.
We recorded the number of times that the visualization with higher deviation was selected as being more interesting, and vice versa (denoted $N_h$ and $N_l$ respectively).

To generate comparable pairs of visualizations, 
we adopted the following procedure.
We randomly chose eight unique <dimension, measure> pairs from the DIAB dataset.
For a selection query on the race attribute, we constructed for each <dimension, measure> pair, three hypothetical visualizations corresponding to varying degrees of deviation between the target and comparison distributions, specifically, low, medium, and high deviation.
We then randomly selected two pairs of visualizations from the three generated above (e.g. high-low and low-medium).
Since both visualizations in each pair showed the same dimension and measure attributes, we minimized the effect of context on participants' preferences.\\
\noindent {\it \underline{Results}}: The 20 MTurk workers took on average 
1 min 46 sec to compare the 16 pairs of visualizations.
The average $N_h$ across all participants was XXX and $N_l$ YYY.
Under our null hypothesis (that deviation is irrelevant to utility of visualization),
there would be no difference between $N_{h}$ and $N_{l}$.
However, we find that the means are different: XXX and YYY with a p-value of ZZZ.
This finding verifies our hypothesis that, on average, subjects 
find visualizations with deviation more interesting and useful.


% Next, for each view, we presented the subject with two visualizations randomly chosen from the three described above.
% The subject was asked to choose the visualization that seemed more
% valuable or interesting.

% Since the axes for each pair of visualizations were identical, we minimized the effect {\it context} would have on user preferences.
% We ran this experiment with 12 subjects.
% Our alternate hypothesis was that visualizations showing large deviation were more interesting than those showing small deviation.
% Table \ref{} shows the results of this experiment.
% $N_{high}$ denotes the number of instances where the subject responded that the visualization with higher deviation was more interesting than the one with low deviation. 
% $N_{low}$ denotes the opposite.
% 

\stitle{Comparing \SeeDB to Baseline Recommendations:}
{\it \underline{Aim}}: Compare whether \SeeDB's visualization 
recommendations 
are more useful than baseline recommendations.
\noindent {\it \underline{Participants}}: 20 participants on MTurk.
\noindent {\it \underline{Methodology}}: We performed a within-subjects study comparing the quality of \SeeDB visualizations (via ratings) to quality of baseline recommendations.
For this experiment, we ran \SeeDB on the DIAB dataset and picked the top-$5$ visualizations returned by the system.
For baseline recommendations, we randomly selected five visualizations from
the full set of visualizations considered by \SeeDB.
% We chose the RANDOM strategy as a baseline to balance any effects of context and user preferences.
Participants were provided a brief background on the DIAB dataset prior to the study.
Participants were then presented two sets of visualizations: the recommendations from \SeeDB and baseline recommendations. 
The order of presentation was randomized to avoid order effects.
For each set of visualizations, participants were asked to rate each visualization on a scale of 1 (lowest) - 5 (highest) based on how {\it interesting, insightful or valuable} they found the visualization.
Participants were also asked to provide an overall rating for each set of visualizations.\\
\noindent {\it \underline{Results}}:The average rating of each visualization generated by \SeeDB was XXX and that for RANDOM is YYY (p-value ZZZ).
In addition, the average rating of the set of visualizations is AAA for \SeeDB and BBB for RANDOM.
This shows that \SeeDB produces recommendations that are significantly better than baseline recommendations.
As we incorporate context and user preferences
into the mix, we expect \SeeDB's visualizations
to become even more useful than they currently are.
% to compare the quality of \SeeDB's recommendations to baseline recommendations.
% We chose the RANDOM strategy from Section \ref{} as our baseline.
% We chose to compare against RANDOM since it provides a worst-case lowerbound on utility, and more importantly, it can balance any effects of context or user history. 

% These visualizations were compared against the visualizations 
% generated from 10 randomly chosen views of the DIAB dataset. 
% Each subject rated the random and \SeeDB visualizations on a scale of 1 (low)--5 (high).
% In addition, each subject provided an overall rating for the set of 10 visualizations.
% Table \ref{} shows the results of this experiment, run with 12 subjects. 


\stitle{Observational Study Results:}
{\it \underline{Aim}}: Determine how data-driven 
recommendations can contribute to holistic visualization recommendation.
\noindent {\it \underline{Participants}}: 5 data analysis experts (MIT grad students with 3+ years data analysis experience).
\noindent {\it \underline{Methodology}}: We performed an observational study evaluating how \SeeDB could benefit data analysis experts in quickly identifying interesting and useful visualizations.
In this experiment, we began with a pre-test where we introduced the participants to the study dataset and asked them to build interesting visualizations using their software of choice (Excel, R or Matlab).
The participants were encouraged to talk aloud about what visualization they were building, why and the process.
% we asked analysts to build interesting visualizations for a dataset in their domain of knowledge.
% The analysts were given an overview of the study dataset and asked to use
% software of their choice (Excel, R, Matlab) to build visualizations of the dataset.
We measured the time taken by each participant to generate what they deemed an interesting visualization.
To control for time, we capped the pre-test at 10 minutes.
Next, the participants were given a tour of \SeeDB and instructed in its use.
Participants were then asked to use \SeeDB to find interesting visualizations of their dataset.
Each participant was asked to evaluate the visualizations generated by \SeeDB and talk aloud about the advantages and shortcomings of \SeeDB as they used the tool.
We recorded all comments, ratings, and suggestions for improvement.\\
% The above experiments evaluate our utility metric and data-driven recommendations. 
% In our observational study, we examined how data-driven recommendations would fit into the data analysis workflow.
% For this, we asked five data analysis experts (graduate students with background in data analysis) to evaluate \SeeDB.
% The experts were given an overview of \SeeDB and asked to use \SeeDB to analyze a dataset within their domain of expertise.
% The experts were asked to talk aloud as they used \SeeDB and identify additional information that would be required to analyze the data. 
\noindent {\it \underline{Results}}:
The major observations from this study were:
\begin{denselist}
\item Participants agreed that building valuable visualizations for data analysis was challenging and time-consuming. In our pre-test, we found that the participants spent 5+ minutes generating the first {\it interesting} visualization of their dataset.
\item In contrast, \SeeDB provided recommendations instantly and produced several of the visualizations that the participants were attempting to create (e.g. for the DIAB dataset, \SeeDB generated an age vs. number of procedures chart that a participant wanted to create in the pre-test).
\item Participants rated \SeeDB visualizations highly and agreed that they provided valuable insights. \mpv{cite the ratings they gave seedb}
\item We found that {\it interactivity} was as important as recommendation quality. Participants wanted to interactively perform perform drill downs and rollups to verify and explore trends.
 % i.e., experts did not take visualizations at face-value.
\item We found that along with data distributions, {\it context and user preferences}  were  important factors in determining visualization utility. A deviation that was interesting for a particular view (e.g. age of patient vs. number of procedures) was not interesting for another view (e.g. severity of disease vs. number of procedures). 
% due to prior knowledge of the expert.
\end{denselist}

% The three set of experiments described above provide an evaluation of the quality of \SeeDB results in a real system.
% We found that deviation-based distance metrics are in fact a good metric to identify utility of visualizations.
% We did however also find that in a few, very specific cases, very similar distributions are also valuable (e.g. when the target and comparison views compare disparate datasets).
% Our randomized controlled experiment shows that \SeeDB performs significantly better than the baseline in producing high utility
% recommendations \mpv{data}.
\noindent Our observational study results indicate that automated visualization recommendations can speed up the analysis process significantly, and can be a useful starting point for individuals unfamiliar with the data.
Data-driven recommendations are particularly useful in the absence of semantic information about the dataset, user preferences and history.
\SeeDB recommendations can be made even more useful by incorporating context into our visualization utility computation and supporting (or integrating with systems that support) interactive data exploration.
% We find that holistic recommendations must take the other recommendation dimensions (e.g. context) into account and the visualization system must make it easy for users to interact with the recommended visualizations.


% We demonstrate the utility of \SeeDB\ through a user study on MTurk as well as
% through in person interviews with data analysis experts. 
% Our goals with the user study were as follows: (1) to validate the \SeeDB\
% technique of identifying interesting views via deviation; (2) to evaluate the
% relative merit of different distance metrics; and (3) to evaluate the \SeeDB\
% tool as a whole.

% To study questions (1) and (2), we created an MTurk study where 30 turkers were
% provided with a questionnaire containing 20 visualizations. These visualizations
% had differing utilities and varying number of distinct values on the X axis
% (see Figure \ref{}) for examples of visualizations. For each visualization, we
% asked subjects to rate (on a scale of 1 to 5) if the visualization showed
% something interesting or insightful about the underlying data. We removed the X
% and Y axis labels to avoid confusing the subjects. The results are shown in
% Table \ref{}. We see that XXX. 

% We also used the data from this study to evaluate the
% merit of different kinds of utility metrics. Specifically, we ranked the set of
% visualizations shown to the subjects using different metrics (see Section
% \ref{}).
% We then compared the ranking of visualizations generated by the metrics to the
% rating provided by the users. 
% To convert the rankings to ratings, we divided the ranked list of visualizations
% into 5 bins. The top ranked bin corresponded to a rating of 5 while the lowest
% ranked bin corresponded to a rating of 1.
% We see that XXX.

% To study the utility of \SeeDB\ as a whole, we conducted in-person interviews
% with XXX experts in data analysis. We adopted the following protocol: 

% \stitle {Scenario 1: Demonstrating Utility.} Attendees are provided with three
% diverse, real-world datasets to explore using \SeeDB. For each dataset,
% attendees can issue ad-hoc or pre-formulated queries to \SeeDB. \SeeDB\ will
% then intelligently explore the view space and optimize query execution to return the
% most interesting visualizations with low latency. Attendees can examine the
% returned queries visually, via the associated view metadata, and via
% drill-downs. To aid the evaluation of visualizations, the demo system will 
% be configured to also show the user ``bad'' views (views with low utility) that were not selected
% by \SeeDB.
% Similarly, we provide pre-selected queries (and
% previously known information about their results) to allow attendees to
% confirm that \SeeDB\ does indeed reproduce known information about these
% queries. Attendees will also be able to experiment with a
% variety of distance metrics for computing utility and observe the effects on the
% resulting views.

% \stitle{Scenario 2: Demonstrating Performance and Optimizations.} This scenario
% will use an enhanced user interface and synthetic datasets mentioned above.
% Attendees will be able to easily experiment with a range of synthetic datasets and input
% queries by adjusting various ``knobs'' such as data size, number of attributes, and
% data distribution. In addition, attendees will also be able to select the
% optimizations that \SeeDB\ applies and observe the effect on response times and
% accuracy.

% Thus, through our demonstration of \SeeDB\, we seek to illustrate that (a) it is
% possible to automate labor-intensive parts of data analysis, (b) aggregate
% and grouping-based views are a powerful means to identify interesting trends
% in data, and (c) the right set of optimizations can enable real-time data
% analysis of large datasets.