%!TEX root=document.tex

\section{User Study}
\label{sec:user_study}

Our \SeeDB prototype currently provides 
visualization recommendations based on the 
deviation between the query and the background data.
Our goal in this section is to study how
far we can go even with these visualization
recommendations, i.e., how useful are the 
visualizations generated by \SeeDB, on an absolute scale
and compared to baselines. 
We achieve this via three user studies.
The first study tests our hypothesis that large 
{\it deviations in distribution} 
lead to high utility visualizations;
the second compares \SeeDB's data-driven recommendations 
to baseline recommendations; and the third
determines how data-driven 
recommendations can contribute to 
holistic visualization recommendations. 


\stitle{Study I: Deviation Between Distributions.}
{\it \underline{Aim}}: Test whether the {\it deviations between distributions} metric leads to useful visualizations.
\noindent {\it \underline{Participants}}: 12 participants on Mechanical Turk (MTurk).
\noindent {\it \underline{Methodology}}: 
We conducted a within-subjects experiment comparing 16 pairs of visualizations generated 
from the DIAB dataset (Section \ref{sec:experiments}) 
where each visualizations in a pair showed the same dimension and measure attributes (thus minimizing effect of context) but had varying degrees of deviation. 
(The procedure for constructing comparable pairs of visualizations is described in the tech report~\cite{seedb-tr}.)
Participants were provided a brief background on the DIAB dataset prior to the study.
They were then presented with 16 pairs of visualizations. 
For each pair, participants were asked to choose the visualization that seemed more {\it interesting, insightful or valuable} to them.
We recorded the number of times that the visualization with higher deviation was selected as being more interesting, and vice versa (denoted $N_h$ and $N_l$ respectively).
\techreport{To generate comparable pairs of visualizations, 
we adopted the following procedure.
We randomly chose eight unique <dimension, measure> pairs from the DIAB dataset.
For a selection query on the race attribute, we constructed for each <dimension, measure> pair, three hypothetical visualizations corresponding to varying degrees of deviation between the target and comparison distributions, specifically, low, medium, and high deviation.
We then randomly selected two pairs of visualizations from the three generated above (e.g. high-low and low-medium).
Since both visualizations in each pair showed the same dimension and measure attributes, we minimized the effect of context on participants' preferences.}\\
\noindent {\it \underline{Results}}: The 12 MTurk workers took on average 
1 min 46 sec to compare the 16 pairs of visualizations.
The average $N_h$ across all participants was 10.08 and $N_l$ was 4.
Under the null hypothesis (that deviation is irrelevant to utility of visualization),
there would be no difference between $N_{h}$ and $N_{l}$.
However, we find that the {\em distribution of $N_{h}$ and $N_{l}$ is statistically different with
p-value 0.01}.
This finding verifies our hypothesis that, on average, participants 
find visualizations with deviation more interesting and useful.


% Next, for each view, we presented the subject with two visualizations randomly chosen from the three described above.
% The subject was asked to choose the visualization that seemed more
% valuable or interesting.

% Since the axes for each pair of visualizations were identical, we minimized the effect {\it context} would have on user preferences.
% We ran this experiment with 12 subjects.
% Our alternate hypothesis was that visualizations showing large deviation were more interesting than those showing small deviation.
% Table \ref{} shows the results of this experiment.
% $N_{high}$ denotes the number of instances where the subject responded that the visualization with higher deviation was more interesting than the one with low deviation. 
% $N_{low}$ denotes the opposite.
% 

\stitle{Study II: Comparison of Recommendations.}
{\it \underline{Aim}}: Determine if \SeeDB's visualization 
recommendations 
are more useful than other visualization schemes.
\noindent {\it \underline{Participants}}: 12 participants on MTurk.
\noindent {\it \underline{Methodology}}: We performed a within-subjects study comparing the quality of \SeeDB visualizations (via ratings) to the quality of baseline recommendations.
For this experiment, we ran \SeeDB on the DIAB dataset and picked the top-$5$ visualizations returned by \SeeDB.
For baseline recommendations, we randomly selected five visualizations from
the full set of visualizations considered by \SeeDB.
Participants were provided a brief background on the DIAB dataset prior to the study.
Participants were then presented two sets of visualizations: the recommendations from \SeeDB and the baseline recommendations. 
The order of presentation was randomized to avoid order effects.
For each set of visualizations, participants were asked to rate each visualization on a scale of 1 (lowest) - 5 (highest) based on how {\it interesting, insightful or valuable} they found the visualization.
Participants were also asked to provide an overall rating for each set of visualizations.\\
\noindent {\it \underline{Results}}: 
% The average rating of visualizations generated by \SeeDB was 3.475 and that for RANDOM was 3.175 (p-value 0.1).
{\em The average rating for the entire set of \SeeDB visualizations was 3.75 while that for RANDOM was 3.125, and this difference was found to be statistically significant (p-value 0.04)}. 
Thus, we find that overall, participants prefer the set of recommendations provided by \SeeDB over baseline recommendations. 
As we incorporate context and user preferences
into the mix, we expect \SeeDB's visualizations
to become even more useful.


\stitle{Study III: Observational Study Results:}
{\it \underline{Aim}}: Determine how data-driven 
recommendations can contribute to holistic visualization recommendation.
\noindent {\it \underline{Participants}}: 5 data analysis experts (MIT grad students with 3+ years data analysis experience).
\noindent {\it \underline{Methodology}}: We performed an observational study evaluating how \SeeDB could benefit data analysis experts in quickly identifying interesting and useful visualizations.
In this experiment, we began with a pre-test where we introduced the participants to the study dataset and asked them to build interesting visualizations using their software of choice (Excel, R or Matlab).
Participants were encouraged to talk aloud about visualizations they were building and why.
We measured the time taken by each participant to generate what they deemed an interesting visualization.
To control for time, we capped the pre-test at 10 minutes.
Next, the participants were given a tour of \SeeDB and instructed in its use.
Participants were then asked to use \SeeDB to find interesting visualizations of their dataset.
Each participant was asked to evaluate the visualizations generated by \SeeDB and talk aloud about its advantages and shortcomings.\\
\noindent {\it \underline{Results}}:
The major observations from this study were:
 (a) Building valuable visualizations for data analysis is challenging and time-consuming. In the pre-test, participants spent 5+ minutes generating their first {\it interesting} visualization.
(b) In contrast, \SeeDB provided recommendations instantly and produced several of the visualizations participants had attempted to create in the pre-study.  
(e.g. for the DIAB dataset, \SeeDB generated an age vs. number of procedures chart that a participant wanted to create in the pre-test).
% \item Participants rated \SeeDB visualizations highly and agreed that they provided valuable insights. \mpv{cite the ratings they gave seedb}
(c) We found that {\it interactivity} was as important as recommendation quality. Participants wanted to interactively perform perform drill downs and rollups to verify and explore trends.
 (d) Along with data distributions, {\it context and user preferences}  were  important factors in determining visualization utility. A deviation that was interesting for a particular pair of attributes (e.g. age of patient vs. number of procedures) was not interesting for another (e.g. severity of disease vs. number of procedures). 
% due to prior knowledge of the expert.

% The three set of experiments described above provide an evaluation of the quality of \SeeDB results in a real system.
% We found that deviation-based distance metrics are in fact a good metric to identify utility of visualizations.
% We did however also find that in a few, very specific cases, very similar distributions are also valuable (e.g. when the target and comparison views compare disparate datasets).
% Our randomized controlled experiment shows that \SeeDB performs significantly better than the baseline in producing high utility
% recommendations \mpv{data}.


\stitle{Conclusions.} Our study indicates that automated visualization recommendations can speed up the analysis process significantly, and can be a useful starting point for individuals unfamiliar with the data.
Data-driven recommendations are particularly useful in the absence of semantic information and user preferences.
We also see some directions for improvement: \SeeDB's recommendations can be made even more useful by incorporating context and supporting interactive data exploration.
% We find that holistic recommendations must take the other recommendation dimensions (e.g. context) into account and the visualization system must make it easy for users to interact with the recommended visualizations.
