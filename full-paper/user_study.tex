%!TEX root=document.tex

\section{User Study}
\label{sec:user_study}

In this section, we describe the results of our user study evaluating the quality of recommendations produced by \SeeDB.
We evaluated our system in three stages. 
First, we tested our hypothesis that {\it deviation in distribution} creates high utility views.
Next, we ran a randomized controlled experiment 
comparing the quality of recommendations of \SeeDB to baseline
recommendations.
Finally, we conducted an observational study with data analysis experts 
to understand how data-driven recommendations can contribute to
making high-quality visualization recommendations.

\stitle{Verifying Deviation-based Metric:}
To test our hypothesis that {\it deviations between distributions} make a visualization valuable, we performed the following experiment.
For the DIAB dataset, we randomly chose a set of ten views.
For each view, we constructed three hypothetical visualizations corresponding views with (a) negligible, (b) moderate, and (c) high deviation in target and comparison distributions.
Next, for each view, we presented the subject with two visualizations randomly chosen from the three described above.
The subject was asked to choose the visualization that seemed more
valuable or interesting.
Our null hypothesis was that there visualizations showing large deviation are equally valuable as visualizations showing small deviation. 
Since the axes for each pair of visualizations were identical, we minimized the effect {\it context} would have on user preferences.
We ran this experiment with 12 subjects.
% Our alternate hypothesis was that visualizations showing large deviation were more interesting than those showing small deviation.
Table \ref{} shows the results of this experiment.
$N_{high}$ denotes the number of instances where the subject responded that the visualization with higher deviation was more interesting than the one with low deviation. 
$N_{low}$ denotes the opposite.
If the null hypothesis is true, $N_{high}$ would be the same as $N_{low}$.
However, we find that the means are different: XXX and YYY with a p-value of ZZZ.
Thus, we find that on average, subjects prefer visualizations with deviation, verifying our hypothesis.\mpv{talk about exception?}

\stitle{Randomized Study of Recommendations:}
Next, we performed a within-subjects study to compare the quality of \SeeDB's recommendations to baseline recommendations.
We chose the RANDOM strategy from Section \ref{} as our baseline.
We chose to compare against RANDOM since it provides a worst-case lowerbound on utility, and more importantly, it can balance any effects of context or user history. 
In our experiment, we ran \SeeDB on the DIAB dataset and picked the top-$10$ visualizations returned by the system.
These visualizations were compared against the visualizations 
generated from 10 randomly chosen views of the DIAB dataset. 
Each subject rated the random and \SeeDB visualizations on a scale of 1 (low)--5 (high).
In addition, each subject provided an overall rating for the set of 10 visualizations.
Table \ref{} shows the results of this experiment, run with 12 subjects. 
The average rating of each visualization generated by \SeeDB is
XXX and that for RANDOM is YYY (p-value ZZZ).
In addition, the average rating of the set of visualizations is AAA for \SeeDB and BBB for RANDOM.
\mpv{so what}

\stitle{Observational Study Results:}
The above experiments evaluate our utility metric and data-driven recommendations. 
In our observational study, we examined how data-driven recommendations would fit into the data analysis workflow.
For this, we asked five data analysis experts (graduate students with background in data analysis) to evaluate \SeeDB.
The experts were given an overview of \SeeDB and asked to use \SeeDB to analyze a dataset within their domain of expertise.
The experts were asked to talk aloud as they used \SeeDB and identify additional information that would be required to analyze the data. 
The major observations were:
\begin{denselist}
\item The experts agreed that building valuable visualizations for data analysis was challenging and time-consuming. In our pre-test, we found that the experts spent 5+ minutes generating the first visualization of their dataset.
\item In contrast, the recommendations of \SeeDB were instantaneous and created several of the visualizations that the experts were attempting to create.
\item The experts rated \SeeDB visualizations highly and agreed that they provided valuable insights.
\item We found that {\it interactivity} was as important as recommendation quality. Experts wanted to interactively perform perform drill downs and rollups to verify trends from visualizations.
 % i.e., experts did not take visualizations at face-value.
\item As expected, we found that along with data distributions, {\it context} was an important factor in determining visualization utility. A trend that was interesting for a particular view (e.g. age of patient vs. number of procedures) was not interesting for another view (e.g. severity of disease vs. number of procedures). 
% due to prior knowledge of the expert.
\end{denselist}

% The three set of experiments described above provide an evaluation of the quality of \SeeDB results in a real system.
% We found that deviation-based distance metrics are in fact a good metric to identify utility of visualizations.
% We did however also find that in a few, very specific cases, very similar distributions are also valuable (e.g. when the target and comparison views compare disparate datasets).
% Our randomized controlled experiment shows that \SeeDB performs significantly better than the baseline in producing high utility
% recommendations \mpv{data}.
Thus, our observational study results indicate that automated visualization recommendations can speed up the analysis process significantly, and that data-driven recommendations can suggest high quality visualizations.
While data-driven recommendations are a good start (particularly in the absence of semantic information or user history), we find that holistic recommendations must take the other recommendation dimensions (e.g. context) into account and the visualization system must make it easy for users to interact with the recommended visualizations.


% We demonstrate the utility of \SeeDB\ through a user study on MTurk as well as
% through in person interviews with data analysis experts. 
% Our goals with the user study were as follows: (1) to validate the \SeeDB\
% technique of identifying interesting views via deviation; (2) to evaluate the
% relative merit of different distance metrics; and (3) to evaluate the \SeeDB\
% tool as a whole.

% To study questions (1) and (2), we created an MTurk study where 30 turkers were
% provided with a questionnaire containing 20 visualizations. These visualizations
% had differing utilities and varying number of distinct values on the X axis
% (see Figure \ref{}) for examples of visualizations. For each visualization, we
% asked subjects to rate (on a scale of 1 to 5) if the visualization showed
% something interesting or insightful about the underlying data. We removed the X
% and Y axis labels to avoid confusing the subjects. The results are shown in
% Table \ref{}. We see that XXX. 

% We also used the data from this study to evaluate the
% merit of different kinds of utility metrics. Specifically, we ranked the set of
% visualizations shown to the subjects using different metrics (see Section
% \ref{}).
% We then compared the ranking of visualizations generated by the metrics to the
% rating provided by the users. 
% To convert the rankings to ratings, we divided the ranked list of visualizations
% into 5 bins. The top ranked bin corresponded to a rating of 5 while the lowest
% ranked bin corresponded to a rating of 1.
% We see that XXX.

% To study the utility of \SeeDB\ as a whole, we conducted in-person interviews
% with XXX experts in data analysis. We adopted the following protocol: 

% \stitle {Scenario 1: Demonstrating Utility.} Attendees are provided with three
% diverse, real-world datasets to explore using \SeeDB. For each dataset,
% attendees can issue ad-hoc or pre-formulated queries to \SeeDB. \SeeDB\ will
% then intelligently explore the view space and optimize query execution to return the
% most interesting visualizations with low latency. Attendees can examine the
% returned queries visually, via the associated view metadata, and via
% drill-downs. To aid the evaluation of visualizations, the demo system will 
% be configured to also show the user ``bad'' views (views with low utility) that were not selected
% by \SeeDB.
% Similarly, we provide pre-selected queries (and
% previously known information about their results) to allow attendees to
% confirm that \SeeDB\ does indeed reproduce known information about these
% queries. Attendees will also be able to experiment with a
% variety of distance metrics for computing utility and observe the effects on the
% resulting views.

% \stitle{Scenario 2: Demonstrating Performance and Optimizations.} This scenario
% will use an enhanced user interface and synthetic datasets mentioned above.
% Attendees will be able to easily experiment with a range of synthetic datasets and input
% queries by adjusting various ``knobs'' such as data size, number of attributes, and
% data distribution. In addition, attendees will also be able to select the
% optimizations that \SeeDB\ applies and observe the effect on response times and
% accuracy.

% Thus, through our demonstration of \SeeDB\, we seek to illustrate that (a) it is
% possible to automate labor-intensive parts of data analysis, (b) aggregate
% and grouping-based views are a powerful means to identify interesting trends
% in data, and (c) the right set of optimizations can enable real-time data
% analysis of large datasets.