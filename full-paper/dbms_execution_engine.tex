%!TEX root=document.tex

\section{{\large \VizRecDB\ } Execution Engine}\label{sec:dbms-exec-engine}
In this section, we discuss the design and implementation of the \VizRecDB\
execution engine. 
The input to the \VizRecDB\ engine is a set of view stubs (triples of the form
$(a, m, f)$, where $a$ is the dimension attribute, $m$ is the measure,
and $f$ is the aggregation function) and the output is the top-$k$ views with the highest utility.
The goals of the \VizRecDB\ engine are two fold:
(1) to efficiently compute the utility of a large number of views, and 
(2) to accurately rank views in order of
utility to find the $k$ views with highest utility.


As discussed in the architecture overview, we explore two distinct
implementations of the \VizRecDB\ execution engine.
Our first design implements the \VizRecDB\ engine as a wrapper on top of a
traditional database system.
This design enables \VizRecDB\ to be used unchanged with a variety of
existing DBMSs.
% Our goal is to study how far we can push existing systems to
% support a \VizRecDB-type workload 
% without making any changes to the underlying DBMS.
Implemented as a wrapper over a DBMS, \VizRecDB\ is limited to using the API
exposed by the DBMS; essentially, 
\VizRecDB\ is limited to opening/closing connections to the
database and executing one or more SQL queries. 
Since we have no control over how queries are executed or access to intermediate
results, {\it our optimizations minimize total execution time by minimizing the
total number of scans of the underlying table}.
% We now discuss the basic framework used by \VizRecDB\ to query the DBMS and various
% optimizations supported by \VizRecDB.

\subsection{Execution in an RDBMS}
\label{sec:basic_framework}
Given a set of view stubs provided to the execution engine, conceptually,
the basic approach proceeds as follows:
(1) for each view, we generate SQL queries for the target and
comparison view---recall that these are aggregate queries, where
the target view is the aggregate query corresponding 
to the visualization being considered, and the comparison
view is the aggregate query that is being compared against,
for the purpose of utility computation;
see Section \ref{sec:problem_statement}, 
(2) we execute each view query independently on the DBMS, 
(3) the results of each query (i.e. the aggregated values) are processed and
normalized to compute the target and comparison distributions, 
(4) we compute utility for each target view 
and select the top-$k$ views with the largest utility,
which are then sent to the frontend.
If the underlying table has $a$ dimension attributes, $m$ measure attributes,
and $f$ aggregation functions, $2\times f \times a \times  m$ queries must be separately executed and their results
processed. Even for modest size tables (1M tuples, $f = 1, a = 50, m=5$), this
technique takes prohibitively long (500s on a row store; Section
\ref{sec:experiments}).
The basic approach is clearly inefficient since it examines every possible view and executes each view
query independently.

\subsection{RDBMS Optimizations} 
\label{sec:dbms_optimizations}
The basic approach discussed above performs poorly because it performs {\it 2
$\times$ number of views} scans of the underlying table. We can do much better
if we can use a single scan to evaluate multiple views simultaneously.
In fact, our ideal operator $\mathcal{O}$ would perform a single scan of the
data, compute all $a \times m \times f$ query results in one pass, and return
only the top-$k$ views.
Current database systems do not support this functionality.
A handful of systems have recently introduced the SQL GROUPING
SETS\footnote{GROUPING SETS allow the simultaneous grouping of query results by
multiple sets of attributes.} functionality that could be used to support this
operation. However, there are a few issues with this: the grouping sets
operator will not scale to tables with a large number of attributes because the 
size of the eventual result maintained in memory will be $2 \times a \times f
\times m$ attributes, multiplied by size of an aggregate distribution.
Furthermore, as we show in Section \ref{sec:in_memory_execution_engine},
evaluating all views for the entire dataset is unnecessary and inefficient. 
We can achieve much better performance by aggressively pruning low-utility
views on the fly.

Without this ideal operator $\mathcal{O}$,
our optimizations minimize table scans in two ways: (1) minimize the total number of
queries by intelligently combining queries, and (2) reduce the total
execution time by running queries in parallel. 
Our DBMS-backed execution engine for \VizRecDB\ is agnostic towards the
particular DBMS used to execute queries.
However, because of significant differences in the way that row stores and
column stores organize data, some of the optimizations described below
will be more powerful in row stores and may actually hurt performance in column
stores. In our experimental evaluation (Section \ref{sec:experiments}), we study
the relative advantages of each of our optimizations for row and column stores.

The optimizations supported by \VizRecDB\ are described below.

\subsubsection {Combine Multiple Aggregates} 
A large number of view queries have the same group-by attribute but different
aggregation attributes. 
Therefore, \VizRecDB\ combines all view queries with the same
group-by attribute into a single, combined view query. For instance, instead of executing
queries for views $(a_1$, $m_1$, $f_1)$, $(a_1$, $m_2$, $f_2)$ \ldots $(a_1$, $m_k$, $f_k)$
independently, we can combine the $n$ views into a single view represented by
$(a_1, \{m_1, m_2\ldots m_k\}$, $\{f_1, f_2\ldots f_k\})$. We demonstrate later
on (Section \ref{sec:expts_dbms_execution_engine})  that this optimization
offers a speed-up roughly proportional to the number of measure attributes.

\subsubsection {Combine Multiple Group-bys}
\label{subsec:mult_gb}
  Similar to the multiple aggregates optimization, another optimization
  supported by \VizRecDB\ is to combine queries with different group-by attributes
  into a single query with multiple group-by attributes.
  For instance, instead of executing queries for views $(a_1$, $m_1$, $f_1)$,
  $(a_2$, $m_1$, $f_1)$ \ldots $(a_n$, $m_1$, $f_1)$ independently, we can
  combine the $n$ views into a single view represented by $(\{a_1, a_2\ldots
  a_n\}$, $m_1$, $f_1)$ and post-process results.

Unlike the previous optimization, where the speed-up is proportional to the
number of view queries combined, in this case, the situation is not as straightforward. 
The reason for this is the following:
since we now store the aggregate for every combination
of $a_1, a_2, \ldots, a_n$, 
the number of aggregates that need to be recorded for 
$n$ views is the number of distinct combinations
of $(a_1, \ldots, a_n)$, which, in the worst case, 
is proportional to $\prod_i |a_i|$.
Thus, the number of aggregates that need to be recorded 
grows exponentially (in the worst case) in the
number of group-by attributes. 
We will show (Section \ref{sec:experiments}) that 
the time to execute a query with multiple group-by attributes does indeed
depend on the number of distinct values present in the resulting 
combination of dimension attributes. 
From a DBMS perspective, this is expected because keeping track of a large
number of aggregates impacts computational time (e.g. for sorting in sort-based aggregate)
as well as temporary storage requirements (e.g. for hashing in hash-based
aggregate) making this technique ineffective for large number of values.

Consequently, when we combine group-by attributes, we must ensure that the
number of distinct values remains {\it small enough}, below a specific 
threshold $\tau$, that we determine based on system parameters.
For simplicity, we ignore the correlations between dimension attribute values,
and work with worst-case estimates. 
The upper limit on the number of distinct values for a given combination of
group-by attributes is given by the product of the number of distinct values
for each attribute.
For example, if we combine three dimension attributes $a_i$, $a_j$ and $a_k$
with $|a_i|$, $|a_j|$ and $|a_k|$ distinct values respectively, the maximum number of
distinct groups is $|a_i|\times |a_j| \times |a_k|$.
 % The number of distinct groups in turn depends on the correlation between
 % values of attributes that are being grouped together. 
 % For instance, if two
 % dimension attributes $a_i$ and $a_j$ have $n_i$ and $n_j$ distinct values
 % respectively and a correlation coefficient of $c$, the number of distinct
 % groups when grouping by both $a_i$ and $a_j$ can be approximated by
 % $n_i$$\ast$$n_j$$\ast$(1-$c$) for $c$$\neq$1 and $n_i$ for $c$=1 ($n_i$ must
 % be equal to $n_j$ in this case).
  
Therefore, our problem can be stated as follows:
\begin{problem}[Optimal Grouping Optimization]
{\em Given a set of dimension attributes $A$ = \{$a_1$\ldots$a_n$\}, divide the
dimension attributes in $A$ into groups $A_1, \ldots, A_l$ (where $A_i$ is some
subset of $A$ and $\bigcap A_i$=$A$) such that the worst-case number of distinct values
for each group is below $\tau$.}
\end{problem}
Notice that the problem as stated above is isomorphic to the NP-Complete
{\em bin-packing} problem~\cite{garey}: to see this, we let each dimension attribute
$a_i$ correspond to an item in the bin-packing problem with weight $\log (|a_i|)$,
and we set the threshold on the bin size to be $\log \tau$,
then packing items into bins is identical to finding groups $A_1, \ldots, A_l$,
such that the worst-case number of distinct values is below $\tau$.
Thus, the problem as stated above is NP-Hard.

We use the standard first-fit algorithm~\cite{first-fit} to find the optimal
grouping of dimension attributes.
The first-fit algorithm works as follows:
on initialization, all groups $A_1, \ldots, $ $A_l$ are empty.
  For each dimension attribute, considered in an arbitrary order, place it in the first group
  $A_i$ that it can ``fit into'', i.e., the worst case
  number of distinct values for that $A_i < \tau$.
  For bin-packing, this algorithm has a guarantee of using up to 1.7X more 
  bins than necessary: here, this translates to up to 1.7X more groups than necessary.
Notice that since this grouping is independent of the input query, we can
perform grouping offline using more computationally expensive techniques as
well.

In Section \ref{XXX} we show that this optimization gives us a speedup of XXX.

% Given that the problem is NP-Hard, we use two strategies to group dimension attributes;
% the first strategy is adapted from the standard first-fit approximation algorithm~\cite{first-fit}
% for bin-packing; the second strategy is adapted from Huffman coding~\cite{huffman-codes}.
  
% \squishlist
% \item {\bf First-Fit}: The algorithm is simple;
% at first, all groups $A_1, \ldots, $ $A_l$ are empty.
% For each dimension attribute, considered in an arbitrary order, place it in the first group
% $A_i$ that it can ``fit into'', i.e., the worst case
% number of distinct values for that $A_i < \tau$.
% For bin-packing, this algorithm has a guarantee of using up to 1.7X more
% bins than necessary: here, this translates to up to 1.7X more groups than necessary.
  % In this trategy, we set an upper limit $V$ on the
  % number of distinct groups that any combination of dimension attributes can
  % produce.
  % Now, suppose that each dimension attribute $a_i$ $\in A$ has $n_i$ distinct
  % values.
  % We cast the problem of finding the optimal grouping of dimension attributes as
  % a version of bin-packing.
  % Let each dimension attribute $a_i$ be an item with volume $log(n_i)$ and
  % suppose that we are given bins of volume $log(V)$.
  % Then finding the optimal grouping of attributes is exactly equivalent to
  % finding the optimal packing of attribute items into the minimum number of
  % bins.
%   \item {\bf Huffman-Grouping}: Here, the algorithm works as follows:
%   we start by assigning each dimension attribute its own group,
%   and maintain these groups in sorted ascending order.
%   At each step, we take the two smallest groups out of this sorted list, combine
%   the dimension attributes in both of them into one new group, and then
%   add this new group to the mix and re-sort.
%   We keep doing this until we cannot combine the two smallest groups any more
%   without violating the threshold $\tau$.
% \squishend
% If this was vanilla bin-packing, the first-fit algorithm would be sufficient to
% find near-optimal groupings. 
% The advantage of having the huffman-grouping algorithm is the following: Instead
% of having a hard threshold $\tau$ on the worst-case number of distinct groups,
% the huffman-grouping algorithm can also adapt to the case where we
% are instead provided a black-box function that, given a particular
% grouping of attributes, returns a value for the ``goodness'' of the combination.
% As we will see in Section~\ref{sec:analytical_model}, 
% we develop an analytical model for the performance (i.e., the execution time)
% of the DBMS-backed execution engine. 
% This model can be used to derive this black-box function.
% Given a black-box function, the huffman-grouping algorithm operates
% as follows: until the ``goodness'' of the grouping keeps increasing 
% (i.e., the predicted execution time keeps decreasing),
% repeatedly combine the two smallest groups into a single group.
% We experimentally evaluate both strategies in Section \ref{sec:experiments}.
% 
% \srm{In 6.2.2., we mention the model.    Are we keeping this?  I think it could be ok to put the details in the appendix if we can show one graph that illustrates that the model can be used to predict how many groups to choose, etc.}

\subsubsection{Combine target and comparison view query}
\label{subsec:target_comparison_view}
Since the target view and comparison views only differ in the subset of data
that the query is executed on, we can easily rewrite these two view queries as
one. For instance, for the target and comparison view queries $Q1$ and $Q2$
shown below, we can add a group by clause to combine the two queries into $Q3$.
\begin{align*} 
Q1 = &{\tt SELECT \ } a, f(m) \ \ {\tt FROM} \  D\  {\tt WHERE \ \ x\ <\ 10\
GROUP \ \ BY} \ a \\
Q2 = &{\tt SELECT \ } a, f(m) \ \ {\tt FROM} \  D\  {\tt GROUP \ \ BY} \ a \\
Q3 = &{\tt SELECT \ } a, f(m), {\tt CASE\ IF\ x\ <\ 10\ THEN\ 1\ ELSE\ 0\
END}\\ 
&as\ group1,\ 1\ as\ group2\\ 
&{\tt FROM} \ D\ {\tt GROUP \ \ BY} \ a,\ group1,\ group2
\end{align*}
% This rewriting allows us to obtain results for both queries in a single table
% scan. The impact of this optimization depends on the selectivity of the
% input query and the presence of indexes. When the input query is less selective,
% the query executor must do more work if the two queries are run separately. In
% contrast, in the presence of an index, running selective queries independently
% may be faster.
% \srm{In 4.2.3., I don't understand why the combined query wouldn't always be faster.  It seems like you are replacing two scans with one, which should just be better.  How could it not be (unless there are very selective filter predicates that aren't show in the example???)}
% \agp{I agree.}

  \subsubsection {Parallel Query Execution}
  \label{subsec:parallel_exec}
  While the above optimizations reduce the number of queries executed, we can
  further speedup \VizRecDB\ processing by executing view queries in parallel. When
  executing queries in parallel, we expect co-executing queries to share pages in the
  buffer pool for scans of the same table, thus reducing disk accesses and
  therefore the total execution time. 
  However, a large number of parallel queries can lead to poor performance for
  several reasons including buffer pool contention, locking and cache line
  contention \cite{Postgres_wiki}. 
  As a result, we must identify the optimal number of parallel queries for our workload.
  
  % We do observe a reduction in the
  %overall latency when a small number of queries are executing in parallel;
  % however, the advantages disappear for larger number of queries running in
  % parallel. We discuss this further in the evaluation subsection.
\subsubsection{Other Optimizations}
To further speedup \VizRecDB we can pre-compute various partial results.
For example, in the case where our comparison view is constructed from the
  entire underlying table (Example 1 in Section \ref{sec:introduction}),
  comparison views are the same irrespective of the input query.
Therefore, we can precompute all possible comparison views once and store
  them for future use. 
%   comparisons. If the dataset has $a$ dimension and
%   $m$ measure attributes, 
%   pre-computing comparison views would add $a \times m$
%   tables. This corresponds to an extra storage of $O(a\times m \times n \times f)$ where $n$
%   is the maximum number of distinct values in any of the $a$ attributes,
%   and $f$ is the number of aggregation functions. 
%   Note that pre-computation cannot be used in situations where the comparison
%   view depends on the target view (Example 2) or is directly specified by the
%   user (Example 3).
%   
Similarly, if we precompute a sample of the table being queried, we can run
all view queries against this table to pick the top-$k$ views and only evaluate
those views on the entire dataset.
We do not experimentally evaluate these optimizations because while these
problems reduce the computation required by \VizRecDB, the
challenges related to evaluating a large number of views (whether limited to
target views or limited to a sample) remain unaddressed.

