%!TEX root=document.tex

\section{Introduction}
\label{sec:introduction}


Data visualization is one of the most common techniques for identifying 
trends and finding anomalies in data.
However, with high-dimensional datasets, identifying visualizations that 
effectively present interesting variations or patterns in the data is a non-trivial task:  a
user typically builds a large number of visualizations optimizing for a range of visualization 
types, aesthetic features, and more
%may have to plot thousands of pairs of attributes against each other 
before arriving at one that shows something valuable.

%The hope is that these recommendations, combined with the current interactivity and ease of use of visualization 
%software, can help analysts quickly glean insights form the data.

In this paper, we tackle the problem of automatically recommending such valuable visualizations.
The problem of recommending visualizations is complicated because a ``good'' visualization needs to take into account several different dimensions, including:
\begin{inparaenum}
\item types and properties of the different attributes in the dataset ({\it metadata}), 
\item visual qualities of the visualization ({\it aesthetics}). 
\item the particular subset of data the user is interested in ({\it query}), 
\item variation in and distribution of the data itself ({\it data distribution}), 
\item meaning and relative importance of attributes in the data ({\it semantics}),  and
\item past history of interactions with this user and other users ({\it user preferences})
\end{inparaenum}

Current visualization systems like Spotfire and Tableau have limited capabilities to recommend visualizations --- they only focus on metadata and aesthetics dimensions, following standard visualization best-practices, e.g., choice of appropriate colors,
rules defining when a bar chart is more appropriate than a trend line, etc.  
No existing system that we are aware of 
incorporates insights about the underlying data, or for that matter, any of the other dimensions into its recommendations.

%There is much room for research into leveraging information along the remaining dimensions to make more holistic
%recommendations.



%Data analysts must sift through very large volumes of data 
%to identify trends, insights, or anomalies.  This task often involves the visual inspection of data, 
%Given the scale of data, and the relative ease and intuitiveness of examining data visually,
%analysts often use visualizations as a tool to identify patterns of interest.
%Consequently, visualization software such as Tableau~\cite{}, Spotfire~\cite{} and Many Eyes~\cite{} 
%has seen unprecedented user adoption~\cite{kristi-tech-report}.
%In spite of user friendly software, however, selecting the ``right'' visualization still remains a laborious and 
%challenging task, particularly for novices unfamiliar with the data\mpv{citation?}. 
%As demonstrated by the paper on the Tableau use, majority of an analyst's time is spent in exploring
%different visualizations.
%To alleviate this problem, major visualization software vendors are attempting to incorporate automatic visualization
%recommendations into their software.
%For instance, Spotfire recently launched a ``Recommendations'' module that uses attribute metadata (e.g. type, number of distinct
%values) to suggest some ``best-practice'' visualizations~\footnote{http://spotfire.tibco.com/recommendations}.
%Similarly, Tableau's Show Me capability recommends a chart type (bar chart, map etc.) that is appropriate
%for the particular view of the data~\cite{DBLP:journals/tvcg/MackinlayHS07}.
%Both of these recommendations are straightforward applications of the best practices of visual design~\cite(); 
%neither incorporates insights about the underlying data or prior user history into its recommendations.
% These recommendations use basic metadata about attributes (categorical or numeric, number of distinct values etc)
% to produce simple visualizations that follow best practices of visual design.
%The ultimate vision for such a recommendation module is that given a dataset or a query, the visualization 
%software can study trends in data and previous interactions on that dataset to present to the user the 
%visualizations it deems most valuable.


% \mpv{Should we add a paragraph and schematic of an ideal system, that has an ensemble of models along one or 
% more of the above dimensions to produce a holistic list of recommendations?}

%In this paper, we develop a system that uses a combination of metadata, query, and statistics to 
%provide high quality, data-driven visualization recommendations.
%We envision our model to be used in conjunction with models currently used for recommendation and models that
%leverage context and user history to produce holisitc recommendations.

\stitle{Data-driven Recommendations in \SeeDB.}
To this end, in this work, we describe a new visualization recommendation engine we are building called {\it \SeeDB}.
Eventually, we aim to address all six dimensions of the above list of dimensions by developing a suite of optimization techniques designed to explore
the entire range of visualizations of a user-supplied data set or query result.  
As a first step in this paper, we develop a general data-driven deviation-based metric that can capture all six dimensions to a limited extent (see Section~\ref{sec:problem_statement} and Section~\ref{sec:discussion}).  
We focus on the query and data distribution aspects as a special case:
these add significant utility, and at the same time significant complexity to 
the visualization recommendation process,
and are particularly relevant when an analyst is approaching a dataset 
for the first time. 
We leverage lessons from metadata and aesthetics-based recommendation
generation from prior work.

% However, because metadata and aesthetics based recommendations have been addressed in prior work, in this paper we focus on {\it data-driven} recommendations that use information about data distribution, metadata and the user query to recommend visualizations.
% Such visualizations are particularly appropriate when
% approaching a dataset for the first time, with limited domain knowledge or historical context.

% that identify unusual variations in the result of the user-supplied query as compared to the underlying data.
% consider the inherent variations in the data from a user-supplied query. 
%   that focuses less on the aesthetics of good visualizations, and more
% on developing general techniques that explore the space
%of possible visualizations and recommend those that highlight data variability in the subset of data specified by a user's query.
%Metadata about a dataset, the user's query and information about data distributions can together provide powerful
%information to identify potentially interesting visualizations for the user.
%We call these recommendations {\it data-driven} since they do not draw upon any semantic information about the dataset;
%they are guided by statistics alone.
%We envision these techniques being incorporated into existing visualization systems that allow users to supply context (via, e.g.,
%interaction), and that focus more on the aesthetics of good visualization.
%As mentioned before, since we envision these recommendations to be augmented with context information (along with
%information about aesthetics), we can attack the problem piecemeal.

%Of course,  however, we note upfront
%that {\it there is a variety of ways to quantifying utility and these merit further exploration}.
%\mpv{In fact, we hope that the techniques and framework we describe in subsequent sections may be a general framework 
%into which we can plug in a variety of metrics.}

% deviation-based 
\stitle{Illustrative Example.}
Before presenting our specific model for evaluating the quality of a visualization, we provide a brief illustrative example.
Consider a smartphone app analytics team that is tasked with studying the metrics for BadApp, a smartphone app that has
poor performance and has received a lot of consumer complaints. 
Suppose that the team uses the AppMetrics database containing metrics such as network usage, 
power consumption, load times etc.
Given the large size of the database (millions of records), an analyst will 
overwhelmingly use visualization software to glean insights into the behavior of BadApp.

In a typical workflow, an analyst would begin by using the program's GUI or a custom query language to execute the equivalent
of the following SQL query and pull all BadApp metrics from the database. 
\noindent 
\begin{align*}
& \tt Q \ \ = \ \ SELECT \ * \ FROM \ \  AppMetrics \ \ WHERE  \ Name=``BadApp"
\end{align*}
Next, the analyst would use an interactive drag-and-drop GUI interface to visualize various metrics of BadApp.
For instance, the analyst may visualize average network usage for BadApp, total crashes grouped by session time,
average load times by carrier, distribution of mobile operating systems, and so on.
Under the hood, these visualization operations are essentially queries to the underlying data store and subsequent graphing of 
the results.
For example, the visualization for average load times by carrier is generated by running an operation equivalent to the
SQL query (Q') shown below.
%The result of this query is a two-column table that is very likely going to be viewed as a bar-char~\cite{vql, kristi}.
Table \ref{tab:staplerX} and Figure \ref{fig:staplerX} respectively show an example of the results of Q' and a potential
visualization.

\noindent
\begin{align*}
& \tt Q' = SELECT \ \ carrier,\ AVG(load\_time) \ \ FROM \ \  AppMetrics \\
& \tt \hspace{20pt} WHERE\ Name=``App\text{-}A" \ \ GROUP  \ \ BY \ \ carrier
\end{align*}

\begin{figure}[h]
\vspace{-10pt}
	\centering
	\begin{subfigure}{0.49\linewidth}
	   \begin{tabular}{cc} \hline
		  Carrier & Load Times (ms) \\ \hline
		  AT\&T & 180.55 \\ \hline
		  Sprint & 90.13 \\ \hline
		  T-Mobile & 122.00 \\ \hline
		  Verizon &  145.50\\ \hline
		  \end{tabular}
		  \caption{Data: Average Load Times by Carrier for BadApp} \label{tab:staplerX}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\centering
		{\includegraphics[width=4cm] {Images/dist1.pdf}}
		\caption{Visualization: Average \\ Load Times by Carrier
		 for BadApp}
		\label{fig:staplerX}
	\end{subfigure}
	
	\centering
	\begin{subfigure}{0.49\linewidth}
		{\includegraphics[width=4cm] {Images/dist2.pdf}}
		\caption{Scenario A: Average Load Times by Carrier}
		\label{fig:staplerX-a}
	\end{subfigure}
	\begin{subfigure}{0.49\linewidth}
		\centering
		{\includegraphics[width=4cm] {Images/dist3.pdf}}
		\caption{Scenario B: Average Load Times by Carrier}
		\label{fig:staplerX-b}
	\end{subfigure}
	\vspace{-10pt}
	\caption{Motivating Example}
	\label{fig:intro}
	\vspace{-15pt}
\end{figure}

\noindent As discussed previously, what might make one of these visualizations valuable depends on a host of factors.
In this work, we primarily focus on a data-driven utility metric that is based on deviation.
Specifically, we posit that a visualization is {\em potentially ``interesting'' if it shows 
a trend in the subset of data selected by the analyst
(i.e., metrics about BadApp)
that deviates from the equivalent trend in the overall dataset (i.e., metrics
about all apps)}.
In our example, the visualization of load times by carrier, as shown in Figure
\ref{fig:staplerX}, would be valuable if the global trend for load times of all
apps showed the {\it opposite} trend (e.g. Figure \ref{fig:staplerX-a}).
However, the same visualization may be uninteresting if the load times of all apps
follow a similar trend (Figure \ref{fig:staplerX-b}).
Due to the scale of data, most common visualizations show aggregate summaries of data
as opposed to individual records (e.g. average sales by state vs. sales of each store 
in every state).
Consequently, our current implementation of \SeeDB\ recommends visualizations that show aggregate 
summaries of data.

Of course, there are a variety of other possible data-driven metrics for quality or utility
of a visualization.
For instance, one might focus on visualizations that show order statistics or anomalies~\cite{DBLP:conf/avi/KandelPPHH12}. 
These would be important for spotting, for example, unusual spikes in machine load.
Similarly, drawing upon the literature on data cubes, one might choose visualizations that highlight aggregates that
are unusual given the remaining values in the cube~\cite{DBLP:conf/vldb/Sarawagi00}.
Finally, one might choose to not aggregate any values but show correlations between attributes by plotting a random
sampling of data-points.  Incorporating these other types of visualizations into our framework is an interesting
direction for future work.

Finally, we note that of the vision for \SeeDB\ has been described in a previous vision paper~\cite{DBLP:conf/vldb/Parameswaran2013} and demo paper~\cite{DBLP:journals/pvldb/VartakMPP14}, but
neither papers presented the specific visualization search techniques or evaluation.


% The majority of visualizations that are generated in visualization systems are based on aggregate summaries of the
% underlying data.
% As a result, the {\it trends} we study are the results of grouping and aggregation applied to a given dataset.


% Thus, the recommendation algorithm used by \SeeDB\ works as follows: given a dataset $D$ and a query $Q$
% indicating the subset of data of interest to the analyst, \SeeDB finds the visualizations of $Q$ that 
% show the highest deviation between trends in $Q$ and trends in $D$. 
% Specifically, \SeeDB considers visualizations that can be constructed via a combinations of grouping and 
% aggregation applied to $Q$.



\stitle{Contributions.}
In this paper, describe \SeeDB,
a visualization recommendation engine that efficiently manages the search
for data-driven visualization recommendations at interactive time scales.
We show that \SeeDB can be implemented on top of a traditional relational database,
allowing it to take advantage of the benefits of the interfaces and features
relation engines expose.
Doing this, however,
also leads to inefficiencies that arise due to the requirement that data be
accessed via SQL.   
We manage these inefficiencies by introducing two classes of optimizations:
{\em sharing-based} optimizations, that try to batch
and share as much computation as possible to minimize the number of SQL queries,
and {\em pruning-based optimizations}, that try to avoid
as much unnecessary work as possible, and discarding candidate aggregate views
that are of low utility.

In summary, the contributions of this paper are:
\begin{denselist}
%  \item We design \SeeDB as a system for data-driven visualization recommendations.
%  We explore and evaluate two distinct implementations of the system, one as a
%  wrapper around a database and another a custom solution (Section~\ref{sec:system_architecture}).
\item We describe a general deviation-based framework 
for evaluating the utility  of a visualization,
and present and evaluate a specific metric that identifies 
aggregate dimensions in a dataset that
show maximal variation (Section~\ref{sec:problem_statement}).
We develop two classes of optimizations to make such aggregates run fast 
in a conventional DBMS.
  \item The first set of techniques
  combine queries and aggregates to minimize the number of queries executed and 
  maximize the sharing of scans between queries, 
  including bin-packing~\cite{garey} algorithms and parallelization
  (Section~\ref{sec:sharing_opt}).
  \item The second set of techniques further optimize the process by adapting techniques 
  from both traditional confidence-interval-based~\cite{hoeffding1963probability} top-$k$ ranking and the
   multi-armed bandit problem~\cite{bandits} 
   to the problem finding the top-$k$ visualizations (Section~\ref{sec:in_memory_execution_engine}).
  % \item We explore visualization pruning techniques based on data distribution
  % to prune visualizations even before they are evaluated by the \SeeDB\ system 
  % (Section~\ref{sec:pruning}).
  \item We evaluate the performance of our optimizations on a range of
  real and synthetic datasets and demonstrate the resulting 40-100X speedup 
  (Section~\ref{sec:experiments}). We also demonstrate that our algorithms
  improve performance while still finding very high or maximum utility visualizations.
  \item We present the results of a user study evaluating the recommendations produced by \SeeDB.
\end{denselist}




