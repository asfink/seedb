\section{Incorportating Custom Engine into a DBMS}
\label{sec:incorporating}

\agp{delete if no space. rather, put it into a techreport flag to be cleaned up later.}
In an ideal solution, we would incorporate the algorithms used in our custom
engine into a DBMS. 
The advantages of integrating into a DBMS are clear: we could take advantage of
the superior data storage and retrieval techniques in databases, and we would
avoid building a one-off, super-specialized system.
However, as the current database API stands, there is no way to share table
scans between operations, keep track of custom statistics or intercept the scan
mid-way to perform pruning.
If we are two implement a \VizRecDB-style operator in a traditional database
system, we would have to support the following operations:
\squishlist
\item Ability to store custom state during aggregation operations (e.g.
distributions corresponding to each view)
\item Ability to compute custom statistics during a table scan (in our case
utilities, their cummulative means and variances)
\item Ability to compute multiple aggregates and group-bys at the same time
(similar to the GROUPING SETS functionality)
\item Ability to intercept the scan periodically (think of a call back or a
sleep functionality) in order to update the custom state
\squishend

These operators can be embedded in the query executor or as part of a UDF.
We lay down these requirements because we expect that not just \VizRecDB\ but
other kinds of similar workloads would also benefit from such an API.
This implementation is out of scope for the current work, but we plan to explore
it in future work.

\section{View Pruning}

Next, the View Generator computes pairwise correlations between subsets of the
entire set of possible views.
To do so, the View Generator computes the aggregate distributions for all views
over the entire data (i.e. the comparison view). 
Once the distributions for all views have been computed, it calculates the
correlation between distributions of views that contain dimension attributes
with the same size (i.e. number of distinct values).
Specifically, since the distributions are numeric, it computes the pairwise
Pearson correlation between the distributions.
Next, it (conceptually) builds a graph of all views. 
The nodes of this graph correspond to views and an edge exists between two nodes
if the correlation between the two views is greater than a threshold (set to 0.95 in
our experiments). 
In this graph, the View Generator then identifies cliques (and almost cliques).
Observe that a clique in this graph is a set of highly correlated views,
and therefore, these views are likely to have similar utility. 
As a result, we select a
single node from each clique as a representative view for that clique and
prune the remaining views. 
We can think of this procedure as clustering views based on similarity and
choosing a representative view form each cluster.
At the end of the offline step, the View Generator stores the list of
viable views that must be evaluated at run time.

When the View Generator is invoked at runtime, it reads the list of viable
views for the table, prunes them further based on the input query (e.g.
attributes present in the where clause of the query should not be present in
any view) and passes the remaining views to the Execution Engine for
evaluation.

In real datasets, we find that the offline processing significantly
reduces the number of views that must be evaluated. 
For example, in the
diabetes dataset discussed in Section \ref{sec:experiments}, offline pruning
reduces the total number of views from XXX to XXX due to our pruning
strategy. Similarly, for the banking dataset, the total views are
reduced from XXX to XXX. In Figure \ref{}, we show graphs generated by the
View Generator for both datasets.
