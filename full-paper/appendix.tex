\section{Incorportating Custom Engine into a DBMS}
\label{sec:incorporating}

\agp{delete if no space. rather, put it into a techreport flag to be cleaned up later.}
In an ideal solution, we would incorporate the algorithms used in our custom
engine into a DBMS. 
The advantages of integrating into a DBMS are clear: we could take advantage of
the superior data storage and retrieval techniques in databases, and we would
avoid building a one-off, super-specialized system.
However, as the current database API stands, there is no way to share table
scans between operations, keep track of custom statistics or intercept the scan
mid-way to perform pruning.
If we are two implement a \VizRecDB-style operator in a traditional database
system, we would have to support the following operations:
\squishlist
\item Ability to store custom state during aggregation operations (e.g.
distributions corresponding to each view)
\item Ability to compute custom statistics during a table scan (in our case
utilities, their cummulative means and variances)
\item Ability to compute multiple aggregates and group-bys at the same time
(similar to the GROUPING SETS functionality)
\item Ability to intercept the scan periodically (think of a call back or a
sleep functionality) in order to update the custom state
\squishend

These operators can be embedded in the query executor or as part of a UDF.
We lay down these requirements because we expect that not just \VizRecDB\ but
other kinds of similar workloads would also benefit from such an API.
This implementation is out of scope for the current work, but we plan to explore
it in future work.

\section{View Pruning}

Next, the View Generator computes pairwise correlations between subsets of the
entire set of possible views.
To do so, the View Generator computes the aggregate distributions for all views
over the entire data (i.e. the comparison view). 
Once the distributions for all views have been computed, it calculates the
correlation between distributions of views that contain dimension attributes
with the same size (i.e. number of distinct values).
Specifically, since the distributions are numeric, it computes the pairwise
Pearson correlation between the distributions.
Next, it (conceptually) builds a graph of all views. 
The nodes of this graph correspond to views and an edge exists between two nodes
if the correlation between the two views is greater than a threshold (set to 0.95 in
our experiments). 
In this graph, the View Generator then identifies cliques (and almost cliques).
Observe that a clique in this graph is a set of highly correlated views,
and therefore, these views are likely to have similar utility. 
As a result, we select a
single node from each clique as a representative view for that clique and
prune the remaining views. 
We can think of this procedure as clustering views based on similarity and
choosing a representative view form each cluster.
At the end of the offline step, the View Generator stores the list of
viable views that must be evaluated at run time.

When the View Generator is invoked at runtime, it reads the list of viable
views for the table, prunes them further based on the input query (e.g.
attributes present in the where clause of the query should not be present in
any view) and passes the remaining views to the Execution Engine for
evaluation.

In real datasets, we find that the offline processing significantly
reduces the number of views that must be evaluated. 
For example, in the
diabetes dataset discussed in Section \ref{sec:experiments}, offline pruning
reduces the total number of views from XXX to XXX due to our pruning
strategy. Similarly, for the banking dataset, the total views are
reduced from XXX to XXX. In Figure \ref{}, we show graphs generated by the
View Generator for both datasets.

\section{Confidence Interval Pruning}
Now, we discuss the assumption that we made early on.
The equation described above apply only in the case
where every time a record is read, an unbiased sample
of the utility is drawn from a distribution which
is assumed to be normal. 
This is not quite the case, since each sample (i.e., each record)
doesn't give us an unbiased sample of the utility;
it instead gives us an unbiased sample for the aggregate
of a single group within the view, thereby indirectly affecting
the utility.
For instance, if we read a record corresponding to 
``Airline = UA'', this affects only the ``UA'' group
for a view where we are estimating flight delay grouped by airline.
We now propose two modifications to the equation above to address this issue.
The first modification we make has to do with how we define utility.
Recall from Section \ref{sec:problem_definition} 
that the utility of a view is
defined as the distance between two distributions: 
the distribution of aggregate values for the
target view and the distribution of aggregate values for the comparison view.
These distributions are in turn tied to the 
number of distinct groups present in
each dimension attribute.
For our purposes, it means that if a dimension attribute has $g$ distinct
groups, then a sample with $x$ rows gives us approximately $\frac{x}{g}$ 
values for each group (assuming uniform distribution).
Said another way, a sample with $x$ rows for the purpose of computing 
utility is essentially only giving us a sample of $\frac{x}{g}$ rows.
So the first modification we 
make to Equation \ref{eq:confidence_interval} is to
replace $m$ by $\frac{m}{G_{max}}$ where $G_{max}$ is the maximum number of
distinct groups present in any dimension attribute.
Second, our mean and variance in the utility needs to be calculated 
over a sample of utilities. 
In our case, since we are not getting samples of utilities, 
and instead getting samples from individual groups that contribute
to utility, we do the following. 
Within a phase, we take the estimate of the utility after every
record is read, and then we compute the mean and variance on these estimates.
Since utility estimates only improve as we get closer to reading 
the entire dataset, we drop the estimates of mean and variance at the end
of every phase; additionally, the number of rows read, $m$, is set to $0$
at the end of each phase as well. This allows us to get estimates
of means and variance, using which, we can apply the standard 
confidence interval bounds.
